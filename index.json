[{"authors":["jing-li"],"categories":null,"content":"Dr. Jing (Jane) Li is the Eduardo D. Glandt Faculty Fellow and Associate Professor (with tenure) at the Department of Electrical and System Engineering and the Department of Computer and Information Science at the University of Pennsylvania. Previously she was the Dugald C. Jackson Assistant Professor at the University of Wisconsin–Madison and a faculty affiliate with the UW-Madison Computer Architecture group and Machine Learning group. She is one of the PIs in SRC JUMP center – Center for Research on Intelligent Storage and Processing-In-Memory (CRISP). She spent her early career at IBM T. J. Watson Research Center as a Research Staff Member after obtaining her PhD degree from Purdue University in 2009.\nShe is attracted to all the big problems she can find in computer system across the stack regardless the specific sub-areas. She is a passionate computer experimentalist and enjoy building real computer systems (both hardware and software). She has made contributions to the following \u0026ldquo;memory-centric\u0026rdquo; areas: 1) domain-specific accelerator and its interaction with emerging memories (HMC/HBM/NVM), 2) programmable in-memory computing architecture enabled by emerging nonvolatile memories (PCM/RRAM), 3) system support (e.g., virtualization) for accelerators (e.g., FPGA), and 4) FPGA-based full system simulation infrastructure (MEG) for memory system research. She has strong ties with leading technology companies and has successful technology transfer experience (\u0026gt;40 issued/pending patents).\nShe is the recipient of prestigious NSF Career Award in 2018, DARPA\u0026rsquo;s Young Faculty Award (one out of 2 in computer area and one out of 26 across all areas in science and technology nationwide, the first awardee in computer engineering and computer science at UW-Madison) in 2016, WARF Innovation Awards (WIA) Finalist (only 6 patented technologies out of 400+ patents got selected university wide), IBM Research Division Outstanding Technical Achievement Award in 2012 for successfully achieving CEO milestone, multiple invention achievement awards and high value patent application awards from IBM from 2010-2014, IBM Ph.D. Fellowship Award in 2008, Meissner Fellowship in 2004 from Purdue University, etc. Her research was reported by Yahoo News, Newegg Business, Digital Trends, etc. And she was featured in Madison Magazine (Channel 3000) as a rising research star.\nShe has been serving on the technical committee for the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), International Symposium on Computer Architecture (ISCA) (both regular and industry tracks), Machine Learning and Systems (MLSys), International Conference for High Performance Computing, Networking, Storage, and Analysis (SC), International Symposium on High Performance Computer Architecture (HPCA), International Symposium on Field-Programmable Gate Arrays (FPGA), International Symposium on Field-Programmable Custom Computing Machines (FCCM), Design Automation Conference (DAC), International Conference on Computer‑Aided Design (ICCAD), IEEE Custom Integrated Circuits Conference (CICC), International Symposium on Low Power Electronics and Design (ISLPED), International Symposium on Microarchitecture (MICRO) (external), IEEE International Symposium on Circuits and Systems (ISCAS), International Electron Devices Meeting (IEDM), etc.. She served as the advisory chair/general chair / technical chair / finance chair / publicity chair for a premier industry memory conference – International Memory Workshop (IMW) and co-organized it with Intel/Micron/SK Hynix/CEA LETI to hold annual meetings with worldwide memory vendors. She is in the Steering/Organizing Committee for IMW, and serves as the Publicity Chair for FPGA\u0026rsquo;20, FPGA\u0026rsquo;19 and ISLPED’18, the Demo Chair for MLSys\u0026rsquo;20, the Best Paper committee Co-Chair for TRETS. She serves as Best Paper committee for several conferences including FCCM and CICC. She is an editor for Journal of Low Power Electronics (JOLPE), and associate editor for Transactions on Reconfigurable Technology and System (TRETS) and IEEE Computer Architecture Letters (CAL). She is serving at ACM SIGDA Technical Committee on FPGAs and Reconfigurable Computing (TC-FPGA).\n","date":1639068276,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1639068276,"objectID":"63412de3bfd3c7aa1f169e816da53be8","permalink":"https://penn-cil.github.io/authors/jing-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jing-li/","section":"authors","summary":"Dr. Jing (Jane) Li is the Eduardo D. Glandt Faculty Fellow and Associate Professor (with tenure) at the Department of Electrical and System Engineering and the Department of Computer and Information Science at the University of Pennsylvania. Previously she was the Dugald C. Jackson Assistant Professor at the University of Wisconsin–Madison and a faculty affiliate with the UW-Madison Computer Architecture group and Machine Learning group. She is one of the PIs in SRC JUMP center – Center for Research on Intelligent Storage and Processing-In-Memory (CRISP).","tags":null,"title":"Jing Li","type":"authors"},{"authors":["jialiang-zhang"],"categories":null,"content":"Jialiang Zhang is a fourth-year PhD student advised by Prof. Jing Li. He received his B.E. degree from University of Electronic Science and Technology of China. His research focuses on hardware acceleration of big data and machine learning applications using FPGA and emerging memory technologies. He is also an expert in high speed digital system and high performance mixed-signal circuit design.\n","date":1628700276,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1628700276,"objectID":"cdc4fe31c5009e9d18ea561d9fdb4fca","permalink":"https://penn-cil.github.io/authors/jialiang-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jialiang-zhang/","section":"authors","summary":"Jialiang Zhang is a fourth-year PhD student advised by Prof. Jing Li. He received his B.E. degree from University of Electronic Science and Technology of China. His research focuses on hardware acceleration of big data and machine learning applications using FPGA and emerging memory technologies. He is also an expert in high speed digital system and high performance mixed-signal circuit design.","tags":null,"title":"Jialiang Zhang","type":"authors"},{"authors":["yue-zha"],"categories":null,"content":"Yue Zha received his B.S. degree in Physics from Peking University in 2013, and M.S. degree in Electrical and Computer Engineering and Computer Science from University of Wisconsin-Madison in 2015. He is currently pursuing his Ph.D. degree at the Department of Electrical and Systems Engineering, University of Pennsylvania.\n","date":1615852800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1615852800,"objectID":"5bfe37aab068a71ae66eab440f3431b9","permalink":"https://penn-cil.github.io/authors/yue-zha/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yue-zha/","section":"authors","summary":"Yue Zha received his B.S. degree in Physics from Peking University in 2013, and M.S. degree in Electrical and Computer Engineering and Computer Science from University of Wisconsin-Madison in 2015. He is currently pursuing his Ph.D. degree at the Department of Electrical and Systems Engineering, University of Pennsylvania.","tags":null,"title":"Yue Zha","type":"authors"},{"authors":["maxwell-strange"],"categories":null,"content":"Maxwell Strange was an undergraduate student at the University of Wisconsin-Madison pursuing degrees in both Computer Engineering and Computer Science. At WiCIL, his current research interests included workload acceleration through software/hardware co-optimization as well as signals applications. He was the president of the IEEE student organization on campus as well.\nMaxwell is now a PhD student at Electrical Engineering Department at Stanford University.\n","date":1519516800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1519516800,"objectID":"a8e9f70282d31969273b847b3113262c","permalink":"https://penn-cil.github.io/authors/maxwell-strange/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/maxwell-strange/","section":"authors","summary":"Maxwell Strange was an undergraduate student at the University of Wisconsin-Madison pursuing degrees in both Computer Engineering and Computer Science. At WiCIL, his current research interests included workload acceleration through software/hardware co-optimization as well as signals applications. He was the president of the IEEE student organization on campus as well.\nMaxwell is now a PhD student at Electrical Engineering Department at Stanford University.","tags":null,"title":"Maxwell Strange","type":"authors"},{"authors":["huaye-li"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"31cb2cfa379abc35b533c53bf33806b1","permalink":"https://penn-cil.github.io/authors/huaye-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/huaye-li/","section":"authors","summary":"","tags":null,"title":"Huaye Li","type":"authors"},{"authors":["nick-beckwith"],"categories":null,"content":"Nick Beckwith is a PhD student at the University of Pennsylvania with the Electrical and Systems Engineering Department. He received his B.S. in Computer Engineering at the University of Wisconsin-Madison in 2019. His primary interests lie in providing research infrastructures for novel cross-stack solutions in embedded systems, emerging memory technologies and tightly integrated heterogeneous systems. He looks forward to tackling the problems of a post-Moore era with these fast, novel and high fidelity research infrastructures.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"39012206d8ccc89029f3a6311f3f6791","permalink":"https://penn-cil.github.io/authors/nick-beckwith/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nick-beckwith/","section":"authors","summary":"Nick Beckwith is a PhD student at the University of Pennsylvania with the Electrical and Systems Engineering Department. He received his B.S. in Computer Engineering at the University of Wisconsin-Madison in 2019. His primary interests lie in providing research infrastructures for novel cross-stack solutions in embedded systems, emerging memory technologies and tightly integrated heterogeneous systems. He looks forward to tackling the problems of a post-Moore era with these fast, novel and high fidelity research infrastructures.","tags":null,"title":"Nick Beckwith","type":"authors"},{"authors":null,"categories":null,"content":"TIME AND PLACE Lectures: M W 12:00pm-1:30pm\nLocation: Towne 319\nUNITS: 1.0 CU\nINSTRUCTOR Jing (Jane) Li (janeli@seas.upenn.edu)\nOffice: Levine 274\nOffice Hours: TBA\nTEACHING ASSISTANTS  TBA TBA  COURSE OVERVIEW Machine learning (ML) techniques are enjoying rapidly increasing adoption in our daily life, due to the synergistic advancements across data, algorithm, and hardware. However, designing and implementing systems that can efficiently support ML models across various deployment scenarios from edge to cloud remains a significant obstacle, in large part due to the gap between machine learning’s promise (core ML algorithm and method) and its real-world utility (diverse and heterogeneous computing platforms).\nThe course is designed to introduce a new engineering discipline at the intersection of machine learning and hardware systems to bridge the gap. The covered topics include basics of deep learning, deep learning frameworks, deep learning on contemporary computing platforms (CPU, GPU, FPGA) and programmable accelerators (TPU), performance measures, numerical representation and customized data types for deep learning, co-optimization of deep learning algorithms and hardware, training for deep learning and support for complex deep learning models. The course is structured with a combination of lectures, labs, research paper reading/in-class discussion, a final project and guest lectures with state-of-the-art industry practices. The goal is to help students to 1) gain hands-on experiences on deploying deep learning models on CPU, GPU and FPGA; 2) develop the intuition on how to perform close-loop co-design of algorithm and hardware through various engineering knobs such as algorithmic transformation, data layout, numerical precision, data reuse, and parallelism for performance optimization given target accuracy metrics, 3) understand future trends and opportunities at the intersection of ML and computer system fields. 4) (For CIS or ML students), gain necessary computer hardware knowledge for algorithm-level optimizations.\nPREREQUISITES CIS 240, or equivalent\nProficiency in programming: ENGR105, CIS110, CIS120, or equivalent. Lab assignments in this course will be based in PyTorch (CPU, GPU) and OpenCL (FPGA). (Note that CIS 371 is not officially required but helpful)\nUndergraduates: Permission of the instructor is required to enroll in this class. If you are unsure whether your background is sufficient for this class, please talk to/email the instructor.\nLab 0 is designed to evaluate if your background is sufficient to take the course.\nGRADING POLICY Lab Assignments = 40%\nFinal Project = 50%\nOthers (Reading, Course Survey) = 10%\nLate policy: Each student will have 7 free “late days” to use during the semester. You can use these late days to submit lab/project after the due date without any penalty. Assignments that are submitted late, after exhausting the quota of late days will result in 50% credit deducted per day, i.e., zero credit after 2 late days. Do not exhaust all the late days on the first lab.\nCollaboration policy: Study groups are allowed, and students may discuss in groups. However, we expect students to understand and complete their own lab assignments. Each student must conduct the lab independently and hand in one lab assignment per student. For the final project, students are expected to work in groups (2 students per group). Each team should turn in one final project report. In the project report, please write down each team member’s specific contribution.\nReading assignment turn-in: The paper review will be turned in via Google form by 11:30am before lecture (link will be posted on the canvas website). Lab/project assignment turn-in: Lab and project reports will be turned in electronically through the Penn Canvas website. Log in to Canvas with your PennKey and password, then select ESE 539 from the Courses and Groups dropdown menu. Submission should be as a single file (preferably .pdf).\nCLASS HOMEPAGE: TBA (a Canvas website will be provided)\nPiazza will be used for discussions and clarifications.\nINVITED SPEAKERS TBA\nCLASS SCHEDULE (TENTATIVE)    Date Topic Course Content Notes/Assignment     09/01 Class Introduction  Lab 0 release   09/08 Introduction to Deep Learning Model, Dataset, Cost (loss) function, Optimizer, Overfitting/Generalization, Regularization    09/13 PyTorch Tutorial  Lab 0 due   09/15 Deep Neural Network Architecture Kernel Computation (Inference), AlexNet, VGG, GoogLeNet, ResNet Lab 1 release   09/20 Deep Learning System: Hardware and Software CPU, GPU, FPGA, TPU, PyTorch, ONNX, MLPerf    09/22 FPGA fudementals  Lab 1 due, Lab 2 release   09/27 OpenCL Tutorial I     09/29 Parellelism Data/Model/Pipeline Parellelism, ILP, DLP, TLP, Roofline, Amdahl\u0026rsquo;s Law    10/04 Mapping and Scheduling I Extended Roofline, Parellelism/Data Reuse, Loop Unrolling/Order/Bound, Spatial/Temporal Choice Lab 2 due, Lab 3 release   10/06 Mapping and Scheduling II Auto Tuning, Optimization for specialized HW, Case studies    10/11 Numerial Precision and Custom Data Type INT, FP, Bfloat16, MS-FP, TF32, DLFloat16, Quantization Process (Mapping/Scaling/Range Calibration)    10/13 Arithmetic Hardware Complexity, Cost, Operator fusion    10/18 OpenCL Tutorial II  Lab 3 due, Lab 4 release   10/20 Co-Design I Dense transformation (Direct Conv, GEMM, FFT, Winograd)    10/25 Co-Design II_part 1 Sparse transformation-I    10/27 Co-Design II_part 2 Sparse transformation-II Lab 4 due   11/01 Co-Design III Compact Models and NAS    11/03 Natural Language Processing RNN, LSTM/GRU, Attention, Transformer Project release   11/08 Project Overview     11/10 Training Neural Network I Backprop, Chain Rule, Kernel Computation (Training)    11/15 Training Neural Network II Distributed Training    11/17 Guest Lecture (TBA)     11/22 No Class Penn on Friday schedule    11/24 Guest Lecture (TBA)     11/29 Guest Lecture (TBA )     12/01 Guest Lecture (TBA)     12/06 Wrap up     12/08 Project Presentation  Project final report (due 12/13)    READING: We will assign one paper to read before most lectures. Several review questions will guide you through the paper reading process. In addition to the paper reading questions, we will also ask you to provide brief course feedback after each lecture to help us make fine-grained adjustment throughout the semester.\nLAB AND PROJECT: In addition to Lab 0 which is mainly used to evaluate your background, we have four more labs (two software labs and two hardware labs) and 1 final project (software/hardware co-design). Lab 1 is a one-week assignment and Lab 2-4 are two-week assignments. Lab 1 and Lab 2 will teach students how to build deep neural network (DNN) models in PyTorch and perform workload analysis on CPU and GPU. These two labs will help students to get familiar with AWS computing environment and navigate/modify the tools to find the performance bottlenecks when running DNN on different computing platforms. Lab 3 and Lab 4 will teach student to implement a core architectural component on FPGA in OpenCL and get familiar with Xilinx Vitis unified software platform. The final project will be 1.5-month long (6 weeks). It requires the students to leverage the key learnings from the 4 labs and perform co-design on hardware and software using the techniques and design options introduced in the course to achieve an end-to-end implementation optimized for inference latency given resource constraint and batch size.\nCOMPARISON TO ESE 532: This course is designed to target broader audience (e.g., CIS or ML students) including but not limited to computer engineering. The course covers a higher level abstraction in the system stack with a focus on a specific application domain - deep learning. The various deep learning-specific topics includes domain-specific framework, algorithmic transformation, numerical precision and customized data type and the consequent optimization opportunities in both algorithm and hardware, etc.. For non-computer engineering students, it provides necessary computer-related knowledge to help develop intuitions on how to design hardware-friendly algorithms. For computer engineering students, it provides an in-depth coverage on the state-of-the-art deep learning techniques (software and hardware). This course is not a pre-requisite for ESE 532 but it motivates and prepares computer engineering students before diving deep into the advanced topics covered in ESE 532.\nCOMPARISON TO ESE 546: This course is more focused on the practical deployment of deep learning in various computing environment (phone, wearable, cloud and supercomputer) via the co-design of hardware and algorithm: 1) design hardware to better support the current and next generation of deep learning models and 2) design algorithms that are hardware friendly and can run efficiently on current and future systems. ESE 546 is more focused on the fundamental principles of deep learning and how to build/train deep neural networks. These two courses are complementary to each other.\nACADEMIC MISCONDUCT: Please refer to Penn\u0026rsquo;s Code of Academic Integrity for more information.\n","date":1629072000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1629140400,"objectID":"de850395770145cde7d6cf7b49fcbf6f","permalink":"https://penn-cil.github.io/courses/ese539_fall2021/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/courses/ese539_fall2021/","section":"courses","summary":"Hardware/Software Co-Design for Machine Learning","tags":null,"title":"ESE 539 - Hardware/Software Co-Design for Machine Learning (Fall 2021)","type":"docs"},{"authors":null,"categories":null,"content":"TIME AND PLACE Lectures: M W 12:00pm-1:30pm\nLocation: Zoom UNITS: 1.0 CU\nINSTRUCTOR Jing (Jane) Li (janeli@seas.upenn.edu)\nOffice: Levine 274\nOffice Hours: 2pm-3pm M\nTEACHING ASSISTANTS  Nick Beckwith (nickbeck@seas.upenn.edu) Stefano Yushinski (ystefano@seas.upenn.edu)  COURSE OVERVIEW Machine learning (ML) techniques are enjoying rapidly increasing adoption in our daily life, due to the synergistic advancements across data, algorithm, and hardware. However, designing and implementing systems that can efficiently support ML models across various deployment scenarios from edge to cloud remains a significant obstacle, in large part due to the gap between machine learning’s promise (core ML algorithm and method) and its real-world utility (diverse and heterogeneous computing platforms).\nThe course is designed to introduce a new engineering discipline at the intersection of machine learning and hardware systems to bridge the gap. The covered topics include basics of deep learning, deep learning frameworks, deep learning on contemporary computing platforms (CPU, GPU, FPGA) and programmable accelerators (TPU), performance measures, numerical representation and customized data types for deep learning, co-optimization of deep learning algorithms and hardware, training for deep learning and support for complex deep learning models. The course is structured with a combination of lectures, labs, research paper reading/in-class discussion, a final project and guest lectures with state-of-the-art industry practices (Amazon, Facebook, Google, Intel, Microsoft, and Xilinx). The goal is to help students to 1) gain hands-on experiences on deploying deep learning models on CPU, GPU and FPGA; 2) develop the intuition on how to perform close-loop co-design of algorithm and hardware through various engineering knobs such as algorithmic transformation, data layout, numerical precision, data reuse, and parallelism for performance optimization given target accuracy metrics, 3) understand future trends and opportunities at the intersection of ML and computer system fields. 4) (For CIS or ML students), gain necessary computer hardware knowledge for algorithm-level optimizations.\nPREREQUISITES CIS 240, or equivalent\nProficiency in programming: ENGR105, CIS110, CIS120, or equivalent. Lab assignments in this course will be based in PyTorch (CPU, GPU) and OpenCL (FPGA). (Note that CIS 371 is not officially required but helpful)\nUndergraduates: Permission of the instructor is required to enroll in this class. If you are unsure whether your background is sufficient for this class, please talk to/email the instructor.\nLab 0 is designed to evaluate if your background is sufficient to take the course.\nGRADING POLICY Lab Assignments = 40%\nFinal Project = 50%\nOthers (Reading, Course Survey) = 10%\nLate policy: Each student will have 5 free “late days” to use during the semester. You can use these late days to submit lab/project after the due date without any penalty. Assignments that are submitted late, after exhausting the quota of late days will result in 50% credit deducted per day, i.e., zero credit after 2 late days. Do not exhaust all the late days on the first lab.\nCollaboration policy: Study groups are allowed, and students may discuss in groups. However, we expect students to understand and complete their own lab assignments. Each student must conduct the lab independently and hand in one lab assignment per student. For the final project, students are expected to work in groups (2 students per group). Each team should turn in one final project report. In the project report, please write down each team member’s specific contribution.\nReading assignment turn-in: The paper review will be turned in via Google form by 11:30am before lecture (link will be posted on the canvas website). Lab/project assignment turn-in: Lab and project reports will be turned in electronically through the Penn Canvas website. Log in to Canvas with your PennKey and password, then select ESE 539 from the Courses and Groups dropdown menu. Submission should be as a single file (preferably .pdf).\nCLASS HOMEPAGE: TBA (a Canvas website will be provided)\nPiazza will be used for discussions and clarifications.\nINVITED SPEAKERS CLASS SCHEDULE (TENTATIVE)    Date Topic Course Content Notes/Assignment     01/20 Class Introduction  Lab 0 release   01/25 Introduction to Deep Learning Model, Dataset, Cost (loss) function, Optimizer, Overfitting/Generalization, Regularization    01/27 PyTorch Tutorial  Lab 0 due; Lab 1 release   02/01 Deep Neural Network Architecture Kernel Computation (Inference), AlexNet, VGG, GoogLeNet, ResNet    02/03 Deep Learning System: Hardware and Software CPU, GPU, FPGA, TPU, PyTorch, ONNX, MLPerf Lab 1 due, Lab 2 release   02/08 FPGA fudementals     02/10 OpenCL Tutorial     02/15 Parellelism Data/Model/Pipeline Parellelism, ILP, DLP, TLP, Roofline, Amdahl\u0026rsquo;s Law    02/17 Mapping and Scheduling I Extended Roofline, Parellelism/Data Reuse, Loop Unrolling/Order/Bound, Spatial/Temporal Choice Lab 2 due, Lab 3 release   02/22 Mapping and Scheduling II Auto Tuning, Optimization for specialized HW, Case studies    02/24 Numerial Precision and Custom Data Type INT, FP, Bfloat16, MS-FP, TF32, DLFloat16, Quantization Process (Mapping/Scaling/Range Calibration)    03/01 Arithmetic Hardware Complexity, Cost, Operator fusion    03/03 Co-Design I Dense transformation (Direct Conv, GEMM, FFT, Winograd) Lab 3 due, Lab 4 release   03/08 Co-Design II_part 1 Sparse transformation    03/10 No Class Spring Break    03/15 Co-Design II_part 2 Sparse transformation    03/17 Co-Design III Compact Models and NAS Lab 4 due   03/22 Natural Language Processing RNN, LSTM/GRU, Attention, Transformer    03/24 Project Overview  Project release   03/29 Training Neural Network I Backprop, Chain Rule, Kernel Computation (Training)    03/31 Training Tutorial Compact Models and NAS    04/05 Training Neural Network II Distributed Training    04/07 Guest Lecture (Derek Chiou, Microsoft) Accelerating the Cloud    04/12 No Class Engagement Day    04/14 Guest Lecture (Eriko Nurvitadhi, Intel) Beyond Peak Performance: Comparing the Real Performance of FPGAs and GPUs on Deep Learning Workloads    04/19 Guest Lecture (Ron Diamant, Randy Huang, Amazon AWS ) Accelerating the Pace of AWS inferentia Chip Development    04/21 Guest Lecture (Yuan Dong Tian, Facebook) Using Machine Learning to learn heuristics for hard optimization problems in computer system design    04/26 Wrap up     04/28 Project Presentation  Project final report (due 05/04)    READING: We will assign one paper to read before most lectures. Several review questions will guide you through the paper reading process. In addition to the paper reading questions, we will also ask you to provide brief course feedback after each lecture to help us make fine-grained adjustment throughout the semester.\nLAB AND PROJECT: In addition to Lab 0 which is mainly used to evaluate your background, we have four more labs (two software labs and two hardware labs) and 1 final project (software/hardware co-design). Lab 1 is a one-week assignment and Lab 2-4 are two-week assignments. Lab 1 and Lab 2 will teach students how to build deep neural network (DNN) models in PyTorch and perform workload analysis on CPU and GPU. These two labs will help students to get familiar with AWS computing environment and navigate/modify the tools to find the performance bottlenecks when running DNN on different computing platforms. Lab 3 and Lab 4 will teach student to implement a core architectural component on FPGA in OpenCL and get familiar with Xilinx Vitis unified software platform. The final project will be 1.5-month long (6 weeks). It requires the students to leverage the key learnings from the 4 labs and perform co-design on hardware and software using the techniques and design options introduced in the course to achieve an end-to-end implementation optimized for inference latency given resource constraint and batch size.\nCOMPARISON TO ESE 532: This course is designed to target broader audience (e.g., CIS or ML students) including but not limited to computer engineering. The course covers a higher level abstraction in the system stack with a focus on a specific application domain - deep learning. The various deep learning-specific topics includes domain-specific framework, algorithmic transformation, numerical precision and customized data type and the consequent optimization opportunities in both algorithm and hardware, etc.. For non-computer engineering students, it provides necessary computer-related knowledge to help develop intuitions on how to design hardware-friendly algorithms. For computer engineering students, it provides an in-depth coverage on the state-of-the-art deep learning techniques (software and hardware). This course is not a pre-requisite for ESE 532 but it motivates and prepares computer engineering students before diving deep into the advanced topics covered in ESE 532.\nCOMPARISON TO ESE 546: This course is more focused on the practical deployment of deep learning in various computing environment (phone, wearable, cloud and supercomputer) via the co-design of hardware and algorithm: 1) design hardware to better support the current and next generation of deep learning models and 2) design algorithms that are hardware friendly and can run efficiently on current and future systems. ESE 546 is more focused on the fundamental principles of deep learning and how to build/train deep neural networks. These two courses are complementary to each other.\nACADEMIC MISCONDUCT: Please refer to Penn\u0026rsquo;s Code of Academic Integrity for more information.\n","date":1586736000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1586804400,"objectID":"e7c15eab46e9bd7760b4722680ea6ba0","permalink":"https://penn-cil.github.io/courses/ese539_spring2021/","publishdate":"2020-04-13T00:00:00Z","relpermalink":"/courses/ese539_spring2021/","section":"courses","summary":"Hardware/Software Co-Design for Machine Learning","tags":null,"title":"ESE 539 - Hardware/Software Co-Design for Machine Learning (Spring 2021)","type":"docs"},{"authors":null,"categories":null,"content":"TIME AND PLACE Lectures: M W 4:30-6:00pm (schedule on Penn InTouch will be updated)\nLocation: TBA\nUNITS: 1.0 CU\nINSTRUCTOR Jing (Jane) Li (janeli@seas.upenn.edu)\nOffice: Levine 274\nOffice Hours: 2pm-3pm W\nTEACHING ASSISTANTS  Jialiang Zhang (jlzhang@seas.upenn.edu) Nick Beckwith (nickbeck@seas.upenn.edu)  COURSE OVERVIEW Machine learning (ML) techniques are enjoying rapidly increasing adoption in our daily life, due to the synergistic advancements across data, algorithm, and hardware. However, designing and implementing systems that can efficiently support ML models across various deployment scenarios from edge to cloud remains a significant obstacle, in large part due to the gap between machine learning’s promise (core ML algorithm and method) and its real-world utility (diverse and heterogeneous computing platforms).\nThe course is designed to introduce a new research area at the intersection of machine learning and hardware systems to bridge the gap. The covered topics include basics of deep learning, deep learning frameworks, deep learning on contemporary computing platforms (CPU, GPU, FPGA) and programmable accelerators (TPU), performance measures, numerical representation and customized data types for deep learning, co-optimization of deep learning algorithms and hardware, training for deep learning and support for complex deep learning models. The course is structured with a combination of lectures, labs, research paper reading/in-class discussion, a final project and guest lectures with state-of-the-art industry practices (Amazon, Facebook, Google, Intel, Microsoft, and Xilinx). The goal is to help students to 1) gain hands-on experiences on deploying deep learning models on CPU, GPU and FPGA; 2) develop the intuition on how to perform close-loop co-design of algorithm and hardware through various engineering knobs such as algorithmic transformation, data layout, numerical precision, data reuse, and parallelism for performance optimization given target accuracy metrics, 3) understand future trends and opportunities at the intersection of ML and computer system fields. 4) (For CIS or ML students), gain necessary computer hardware knowledge for algorithm-level optimizations.\nPREREQUISITES CIS 240, or equivalent\nProficiency in programming: ENGR105, CIS110, CIS120, or equivalent. Lab assignments in this course will be based in PyTorch (CPU, GPU) and OpenCL (FPGA). (Note that CIS 371 is not officially required but helpful)\nUndergraduates: Permission of the instructor is required to enroll in this class. If you are unsure whether your background is sufficient for this class, please talk to/email the instructor.\nGRADING POLICY Lab Assignments = 40%\nFinal Project = 50%\nReading = 10%\nLate policy: Each student will have 5 free “late days” to use during the semester. You can use these late days to submit lab/project after the due date without any penalty. Assignments that are submitted late, after exhausting the quota of late days will result in 50% credit deducted per day, i.e., zero credit after 2 late days. Do not exhaust all the late days on the first lab.\nCollaboration policy: Study groups are allowed, and students may discuss in groups. However, we expect students to understand and complete their own lab assignments. Each student must conduct the lab independently and hand in one lab assignment per student. For the final project, students are expected to work in groups (2 students per group). Each team should turn in one final project report. In the project report, please write down each team member’s specific contribution.\nReading assignment turn-in: The paper review will be turned in via Google form by 3:00pm before lecture (link will be posted on the canvas website). Lab/project assignment turn-in: Lab and project reports will be turned in electronically through the Penn Canvas website. Log in to Canvas with your PennKey and password, then select ESE 680 from the Courses and Groups dropdown menu. Submission should be as a single file (preferably .pdf).\nCLASS HOMEPAGE: TBA (a Canvas website will be provided)\nPiazza will be used for discussions and clarifications.\nINVITED SPEAKERS CLASS SCHEDULE (TENTATIVE)    Date Topic Course Content Notes/Assignment Due     09/02 Class Introduction  09/07 no class   09/09 Introduction to Deep Learning Model, Dataset, Cost (loss) function, Optimization    09/14 Deep Neural Network Architecture Kernel Computation (Inference), AlexNet, VGG, GoogLeNet, ResNet    09/16 Deep Learning System: Hardware and Software CPU, GPU, FPGA, TPU, PyTorch, ONNX, MLPerf    09/21 PyTorch Tutorial  Lab 1 (due 9/28)   09/23 FPGA fudementals     09/28 Guest Lecture (Cliff Young, Google) Neural Networks Have Rebooted Computer Architecture: What Should We Reboot Next? Lab 2 (due 10/07)   09/30 Parellelism Data/Model/Pipeline Parellelism, ILP, DLP, TLP    10/05 Mapping and Scheduling I Roofline, Parellelism/Data Reuse, Loop Unrolling/Order/Bound, Spatial/Temporal Choice    10/07 OpenCL Tutorial  Lab 3 (due10/19)   10/12 Mapping and Scheduling II Auto Tuning, Optimization for specialized HW, Case studies    10/14 Guest Lecture (Stephen Neuendorffer, Xilinx) Optimizing data movement for Versal AI Engine    10/19 Numerial Precision and Custom Data Type INT, FP, Bfloat16, MS-FP, TF32, DLFloat16, Quantization Process (Mapping/Scaling/Range Calibration) Lab 4 (due 11/02)   10/21 Guest Lecture (Randy Huang, Amazon) Accelerating the Pace of AWS Inferentia Chip Development: From Concept to End Customers Use    10/26 Project Overview  Project release   10/28 Guest Lecture (Eric Chung, Microsoft) TBD    11/02 Arithmetic Hardware Complexity, Cost    11/04 Guest Lecture (Martin Langhammer, Intel) Low Precision Arithmetic in FPGAs    11/09 Co-Design I Dense transformation (Direct Conv, GEMM, FFT, Winograd)    11/11 Co-Design II Sparse transformation    11/16 Co-Design III Compact Models and NAS    11/18 Guest Lecture (Hsien-Hsin Sean Lee, Facebook) TBD    11/23 Natural Language Processing RNN, LSTM, Attention, Transformer    11/25 Training Neural Network I Backprop, Hyper-parameter    11/30 Training Neural Network II SGD variants, Kernel Computation (Training), Cost Analysis    12/02 Training Neural Network III Distributed Training    12/07 Wrap up     12/09 Project Presentation  Project final report (due 12/15)    READING: We will assign one paper to read before each lecture (9/9-11/18). Several review questions will guide you through the paper reading process. In addition to the paper reading questions, we will also ask you to provide brief course feedback after each lecture to help us make fine-grained adjustment throughout the semester.\nLAB AND PROJECT: We have four labs (two software labs and two hardware labs) and 1 final project (software/hardware co-design). Lab 1 is a one-week assignment and Lab 2-4 are two-week assignments. Lab 1 and Lab 2 will teach students how to build deep neural network (DNN) models in PyTorch and perform workload analysis on CPU and GPU. These two labs will help students to get familiar with AWS computing environment and navigate the tools to find the performance bottlenecks when running DNN on different computing platforms. Lab 3 will teach student to implement a core architectural component on FPGA in OpenCL and get familiar with Xilinx Vitis unified software platform. Lab 4 consists of two parts: 1) performing software experiments in PyTorch to study the impact of low-precision arithmetic on inference accuracy and 2) performing hardware experiments in OpenCL to study the latency and the resource utilization of low-precision arithmetic hardware. The final project will be 1.5-month long (6 weeks). It requires the students to leverage the key learnings from the 4 labs and perform co-design on hardware and software using the techniques and design options introduced in the course to achieve an end-to-end implementation optimized for single-batch inference latency given an accuracy target.\nCOMPARISON TO ESE 532: This course is designed to target broader audience (e.g., CIS or ML students) including but not limited to computer engineering. The course focuses on a specific application domain - deep learning and provides an in-depth coverage on various deep learning-specific topics e.g., numerical precision and customized data type and the consequent optimization opportunities in both hardware and algorithm, etc.. For non-computer engineering students, it provides necessary computer-related knowledge to help develop intuitions on how to design hardware-friendly algorithms. For computer engineering students, it provides an in-depth coverage on the state-of-the-art deep learning techniques (software and hardware). This course is not a pre-requisite for ESE 532 but it motivates and prepares computer engineering students before diving deep into the advanced topics covered in ESE 532.\nCOMPARISON TO ESE 546: This course is more focused on the practical deployment of deep learning in various computing environment (phone, wearable, cloud and supercomputer) via the co-design of hardware and algorithm: 1) design hardware to better support the current and next generation of deep learning models and 2) design algorithms that are hardware friendly and can run efficiently on current and future systems. ESE 546 is more focused on the fundamental principles of deep learning and how to build/train deep neural networks. These two courses are complementary to each other.\nACADEMIC MISCONDUCT: Please refer to Penn\u0026rsquo;s Code of Academic Integrity for more information.\n","date":1586736000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1586804400,"objectID":"ecaa5816c563305b983f1c9ac4a807c0","permalink":"https://penn-cil.github.io/courses/ese680_fall2020/","publishdate":"2020-04-13T00:00:00Z","relpermalink":"/courses/ese680_fall2020/","section":"courses","summary":"Hardware/Software Co-Design for Machine Learning","tags":null,"title":"ESE 680 - Hardware/Software Co-Design for Machine Learning (Fall 2020)","type":"docs"},{"authors":null,"categories":null,"content":"At PennCIL, our research focuses on future systems in two general directions:\n Augmenting computer systems with domain specific accelerators and emerging memories \u0026ndash; an essential step towards next-generation systems. It requires modest system changes and has low deployment barrier but the performance gain might be potentially limited.   Architecture/Algorithm Co-design  Large scale graph analytics Deep learning and AI   Mechanisms and Abstractions for Virtualization in Cloud Open Source Computer System Infrastructure  Rethinking computer systems with post-CMOS technology \u0026ndash; a more aggressive approach in pushing innovations across the entire system stack. It could lead to dramatic improvement in performance but often requires more disruptive change in both hardware and software.   Liquid Silicon Two-Dimensional Associative Processor  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"bc8c75ec10f4acb491c3299f2f23af66","permalink":"https://penn-cil.github.io/research/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/research/","section":"research","summary":"Documentation for all research activities at PennCIL.","tags":null,"title":"Research Overview","type":"docs"},{"authors":["Yue Zha"],"categories":[],"content":"Liquid Silicon (L-Si) is a general-purpose in-memory computing architecture with complete system support that addresses several key fundamental limitations of state-of-the-art reconfigurable data-flow architectures (including FPGA, TPU, CGRA, etc.) in supporting emerging machine learning and big data applications. As compared with most projects in literature which focus on part of the system stack, L-Si is a full stack solution that comprises architecture [Zha2018FPGA], compiler [Zha2016ICCAD], programming model and system integration [Zha2018ASPLOS], with a real chip demonstration [Zha2019VLSIC]. The computing model of L-Si is radically different from state-of-the-art reconfigurable data-flow architectures. It maximally reuses the memory array itself (instead of placing computation units near the array) to perform a) heavy weight computation (logic), b) light weight computation(search), c) data storage (memory), and d) communication (routing), with minimal requirement in CMOS supporting circuitry, which can thus be further placed beneath the array. Therefore, it inherits the great benefits of semiconductor memory in integration density and cost, and offers orders of magnitude more parallel data processing capability in situ in the memory array than the best-known solution today. For the first time, it fundamentally blurs the boundary between computation and storage, by exploiting a continuum of general-purpose in-memory compute capabilities across the whole spectrum, from full memory to full computation, or intermediate states in between (partial memory and partial computation). Thus, it provides programmers (or compiler) more flexibility to customize hardware’s compute and memory resources to better match applications needs for higher performance and energy efficiency. We leverage such unique property and provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. We also provide scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization when combined with our system stack.\nTimeline of L-Si project\nTo prove the feasibility of L-Si, we fabricated a test chip in 130 nm CMOS process with HfO2 RRAM – the first real-chip demonstration for general purpose in-memory computing using RRAM.\nDie Photo and Chip Characteristics of L-Si\nWith proposed system support, we evaluated a broad class of legacy and emerging machine learning workloads. Our measurement confirmed the chip operates reliably at low voltage of 650 mV when running these workloads. It achieves 60.9 TOPS/W in performing neural network inferences and 480 GOPS/W in performing high-dimensional similarity search (a key big data application) at nominal voltage supply of 1.2V, showing \u0026gt; 3x and ~100x power efficiency improvement over the state-of-the-art domain-specific CMOS-/RRAM-based accelerators without sacrificing the programmability. In addition, it outperforms the latest nonvolatile FPGA in energy efficiency by \u0026gt; 3x in general compute-intensive applications. As L-Si is a fundamental new computing technology, moving further, we will explore how to scale it up to warehouse computers and scale it down to IoT devices by further specializing the software/hardware stacks.\nComparing L-Si with State-of-the-Art\n","date":1565989140,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989140,"objectID":"f6d1369a71c235ee1ac5c886628f7e5d","permalink":"https://penn-cil.github.io/project/liquid-silicon/","publishdate":"2019-08-16T15:59:00-05:00","relpermalink":"/project/liquid-silicon/","section":"project","summary":"A general-purpose in-memory computing architecture that addresses several key fundamental limitations of state-of-the-art reconfigurable data-flow architectures in supporting emerging machine learning and big data applications","tags":["liquid siicon","processing-in-memory","machine learning","big data","general purpose computing","reconfigurable"],"title":"Liquid Silicon","type":"project"},{"authors":null,"categories":null,"content":"In this research theme, we organize our research around three high-level questions: 1) How to design domain specific accelerators for big data and machine learning applications? 2) What kind of system abstractions are needed to convert hardware specialization into direct benefits that can actually be realized by end-applications and “everyday” programmers? 3) How to effectively evaluate these cross-stack system methods with high fidelity, fairness, and performance? In this research, we aim to answer these questions via the following three paths: 1) architecture/algorithm co-design, 2) mechanisms and abstractions for virtualization, and 3) open-source system research infrastructure.\nArchitecture/Algorithm Co-design Large Scale Graph Analytics Extremely large, sparse graphs with billions of nodes and hundreds of billions of edges arise in many important problem domains ranging from social science, bioinformatics, to video content analysis and search engines. In response to the increasingly larger and more diverse graphs, and the critical need of analyzing them, we focus on large scale graph analytics, an essential class of big data analysis, to explore the comprehensive relationship among a vast collection of interconnected entities. However, it is challenging for existing computer systems to process the massive-scale real-world graphs, not only due to their large memory footprint, but also that most graph algorithms entail irregular memory access patterns and a low compute-to-memory access ratio.\nIn this research, we invented \u0026ldquo;degree-aware\u0026rdquo; hardware/software techniques to improve graph processing efficiency. Our research is built atop a key insight that we obtained from architecture-independent algorithm analysis, which has not been revealed in prior work. More specifically, we identified that a key challenge in processing massive-scale graphs is the redundant graph computations caused by the presence of high-degree vertices which not only increase the total amount of computations but also incur unnecessary random data access. To address this challenge, we developed variants of graph processing systems on an FPGA-HMC platform [Zhang2018FPGA-Graph, Khoram2018FPGA, Zhang2017FPGA-BFS]. For the first time, we leverage the inherent graph property i.e. vertex degree to co-optimize algorithm and hardware architecture. In particular, the unique contributions we made include two algorithm optimization techniques: degree-aware adjacency list reordering and degree-aware vertex index sorting. The former reduces the number of redundant graph computations, while the latter creates a strong correlation between vertex index and data access frequency, which can be effectively applied to guide the hardware design. Further, by leveraging the strong correlation between vertex index and data access frequency created by degree-aware vertex index sorting, we developed two platform-dependent hardware optimization techniques, namely degree-aware data placement and degree-aware adjacency list compression. These two techniques together substantially reduce the amount of external memory access. Finally, we completed the full system design on an FPGA-HMC platform to verify the effectiveness of these techniques. Our implementation achieved the highest performance (45.8 billion traversed edges per second) among existing FPGA-based graph processing systems and was ranked No. 1 on GreenGraph500 list. Green Graph500 (updated June 19, 2019)\nDeep Leaning and AI Deep neural networks (DNNs) deliver impressive results for a variety of challenging tasks in computer vision, speech recognition, and natural language processing, at the cost of higher computational complexity and larger model size. To reduce the load of taxing DNN infrastructures, a number of FPGA-based DNN accelerators have been proposed via new micro-architectures, data ow optimizations, or algorithmic transformation. Due to the extremely large design space, it is challenging to attain good insights on how to design optimal accelerators on a target FPGA.\nIn this research, we take an alternative, and more principled approach to guide accelerator architecture design and optimization. We borrow the insights from the roofline model and further improve it by taking both on-chip and off-chip memory bandwidth into consideration. We apply the model to quantify the difference between available resources provided by native hardware (FPGA devices) and actual resources demanded by the application (CNN classification kernel). To tackle the problem, we develop a number of hardware/software techniques and implement them on FPGA [Zhang2017FPGA-CNN]. The demonstrated accelerator was recognized as the highest performance and the most energy efficient accelerator for dense convolutional neural network (CNN) compared to the state-of-the-art FPGA-based designs.\nMechanisms and Abstractions for Virtualization in Cloud While traditionally being used in embedded systems, custom silicon (e.g., FPGAs) has recently begun to make their way into data centers and the cloud (Amazon AWS EC2 F1 instances, Microsoft Brainwave, Google TPU etc.). While these programmable data-flow architectures provide the lucrative benefits of fine-grained parallelism and high flexibility to accelerate a wide spectrum of applications, system support for them in the context of multi-tenant cloud environment, however, is in its infancy and has two major limitations, 1) inefficient resource management due to the tight coupling between compilation and runtime management, and 2) high programming complexity when exploiting scale-out acceleration, for which the root cause is that hardware resources are not virtualized.\nIn this research, we take FPGA as a case study and develop a full stack solution that can address these limitations and thus, enable virtualization of FPGA clusters in multi-tenant cloud computing environment. Specifically, the key contribution is a new system abstraction that can effectively decouple the compilation and runtime resource management. It allows applications to be compiled offline onto the proposed abstraction and resource allocation to be dynamically determined at runtime. Moreover, it creates an illusion of a single/large virtual FPGA to users, thereby reducing the programming complexity and supporting scale-out acceleration. It also provides virtualization support for the peripheral components (e.g. on-board DRAM, Ethernet), as well as protection and isolation support to ensure a secure execution in multi-tenant cloud environment.\nOpen Source Computer System Infrastructure Hardware-software co-design studies targeting next-generation computer systems that are equipped with emerging memories (HBM/HMC, NVM, etc.) are fundamentally hampered by a lack of scalable, performant and accurate simulation platform. Using software simulator is fundamentally bottlenecked by the low simulation speed and low fidelity, making it impractical to run realistic software stack. Such a challenge has limited effective cross-stack innovations (across computer architecture, OS, compiler, machine learning, and domain sciences). Past efforts have largely focused on simulation infrastructure for processor cores with highly simplified models of the memory subsystem. As a key contribution to the community (including SRC JUMP centers, broad academia and industry) to better drive cross-stack memory system research, we have been developing MEG [Zhang2019FCCM] – an open source FPGA- based simulation platform that enables cycle-exact micro-architecture simulation for computer systems with a special focus on memory subsystem (heterogeneous memory, near/in-memory acceleration). In MEG, we combine silicon- proven RTL design of RISC-V cores with configurable heterogeneous memory subsystems (HMC/HBM/NVM). It is capable of running realistic software stacks including booting Linux and comprehensive application software with high fidelity (cycle-exact microarchitectural models derived from synthesizable RTL), flexibility (modifiable to include custom RTL user IP and/or more abstract models), reproducibility, observability, target software support and performance. MEG can be an effective complement to previous RAMP and more recent Firesim projects in covering memory space and a contribution to the open source RISC-V community.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"5a28c61b77472812c3cf04bbeba5c2e7","permalink":"https://penn-cil.github.io/research/augmenting-computer-system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/research/augmenting-computer-system/","section":"research","summary":"In this research theme, we organize our research around three high-level questions: 1) How to design domain specific accelerators for big data and machine learning applications? 2) What kind of system abstractions are needed to convert hardware specialization into direct benefits that can actually be realized by end-applications and “everyday” programmers? 3) How to effectively evaluate these cross-stack system methods with high fidelity, fairness, and performance? In this research, we aim to answer these questions via the following three paths: 1) architecture/algorithm co-design, 2) mechanisms and abstractions for virtualization, and 3) open-source system research infrastructure.","tags":null,"title":"Augmenting Computer Systems with Domain Specific Accelerators/Emerging Memories","type":"docs"},{"authors":null,"categories":null,"content":"My CV\nLooking forward, we believe more disruptive approaches are needed to fundamentally re-think about how we build computers to address von Neumann bottleneck. We envision future computer chips will not solely be made by Si but rather can utilize and optimize the best of various post-CMOS technologies (including but not limited to novel 1D/2D devices, spintronics, resistive switching devices, quantum electrical/optical device, etc.), to effectively complement silicon CMOS in providing auxiliary/ancillary functions (or cost benefits) that otherwise cannot be easily achieved with silicon CMOS. In this research, we take emerging nonvolatile memory technology (e.g., RRAM) as a case study to showcase two general-purpose in-memory computing architectures based on two radically different non-von Neumann machine models. Note that these two architectures are fundamentally different from prior rich literature of applying memory array as domain-specific dot-product computing unit.\nLiquid Silicon Liquid Silicon (L-Si) is a general-purpose in-memory computing architecture with complete system support that addresses several key fundamental limitations of state-of-the-art reconfigurable data-flow architectures (including FPGA, TPU, CGRA, etc.) in supporting emerging machine learning and big data applications. As compared with most projects in literature which focus on part of the system stack, L-Si is a full stack solution that comprises architecture [Zha2018FPGA], compiler [Zha2016ICCAD], programming model and system integration [Zha2018ASPLOS], with a real chip demonstration Zha2019VLSIC https://penn-cil.github.io/publication/zha-2019-vlsic/. The computing model of L-Si is radically different from state-of-the-art reconfigurable data-flow architectures. It maximally reuses the memory array itself (instead of placing computation units near the array) to perform a) heavy weight computation (logic), b) light weight computation(search), c) data storage (memory), and d) communication (routing), with minimal requirement in CMOS supporting circuitry, which can thus be further placed beneath the array. Therefore, it inherits the great benefits of semiconductor memory in integration density and cost, and offers orders of magnitude more parallel data processing capability in situ in the memory array than the best-known solution today. For the first time, it fundamentally blurs the boundary between computation and storage, by exploiting a continuum of general-purpose in-memory compute capabilities across the whole spectrum, from full memory to full computation, or intermediate states in between (partial memory and partial computation). Thus, it provides programmers (or compiler) more flexibility to customize hardware’s compute and memory resources to better match applications needs for higher performance and energy efficiency. We leverage such unique property and provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. We also provide scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization when combined with our system stack.\nTimeline of L-Si project\nTo prove the feasibility of L-Si, we fabricated a test chip in 130 nm CMOS process with HfO2 RRAM – the first real-chip demonstration for general purpose in-memory computing using RRAM.\nDie Photo and Chip Characteristics of L-Si\nWith proposed system support, we evaluated a broad class of legacy and emerging machine learning workloads. Our measurement confirmed the chip operates reliably at low voltage of 650 mV when running these workloads. It achieves 60.9 TOPS/W in performing neural network inferences and 480 GOPS/W in performing high-dimensional similarity search (a key big data application) at nominal voltage supply of 1.2V, showing \u0026gt; 3x and ~100x power efficiency improvement over the state-of-the-art domain-specific CMOS-/RRAM-based accelerators without sacrificing the programmability. In addition, it outperforms the latest nonvolatile FPGA in energy efficiency by \u0026gt; 3x in general compute-intensive applications. As L-Si is a fundamental new computing technology, moving further, we will explore how to scale it up to warehouse computers and scale it down to IoT devices by further specializing the software/hardware stacks.\nComparing L-Si with State-of-the-Art\nTwo-Dimensional Associative Processor The research project, titled \u0026ldquo;Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box\u0026quot;, won the NSF CAREER Award in 2018. In this research, we developed a radically new computing paradigm, namely two-dimensional associative processing (2D AP) to further advance our previous FPGA-based graph processing architectures and fundamentally address their limitations. Mathematically, 2D AP is a new general-purpose computing model that exploits an extra dimension of parallelism (both intra-word and inter-word parallelism) to accelerate computation as compared with traditional AP which only exploit inter-word parallelism. It is particularly beneficial for massive-scale graph processing. For the first time, we provide a theoretical proof that 2D AP is inherently more efficient as measured by \u0026ldquo;architecturally determined complexity\u0026rdquo; in runtime/area/energy than both von Neumann architecture and traditional AP paradigm in performing graph computation. We also provide detailed micro-architectures and circuits to best implement the proposed computing model, with domain-special language support. A preliminary published version of 2D AP [Khoram2018CAL] was recognized as best of CAL (IEEE Computer Architecture Letters) in 2018.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"196fa92a9dce687bf98cd7508dd76642","permalink":"https://penn-cil.github.io/research/rethinking-computer-system/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/research/rethinking-computer-system/","section":"research","summary":"My CV\nLooking forward, we believe more disruptive approaches are needed to fundamentally re-think about how we build computers to address von Neumann bottleneck. We envision future computer chips will not solely be made by Si but rather can utilize and optimize the best of various post-CMOS technologies (including but not limited to novel 1D/2D devices, spintronics, resistive switching devices, quantum electrical/optical device, etc.), to effectively complement silicon CMOS in providing auxiliary/ancillary functions (or cost benefits) that otherwise cannot be easily achieved with silicon CMOS.","tags":null,"title":"Rethinking Computer Systems with Post-CMOS Technology","type":"docs"},{"authors":["Jing Li"],"categories":[],"content":"The research project, titled \u0026ldquo;Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box\u0026quot;, won the NSF CAREER Award in 2018. In this research, we developed a radically new computing paradigm, namely two-dimensional associative processing (2D AP) to further advance our previous FPGA-based graph processing architectures and fundamentally address their limitations. Mathematically, 2D AP is a new general-purpose computing model that exploits an extra dimension of parallelism (both intra-word and inter-word parallelism) to accelerate computation as compared with traditional AP which only exploit inter-word parallelism. It is particularly beneficial for massive-scale graph processing. For the first time, we provide a theoretical proof that 2D AP is inherently more efficient as measured by \u0026ldquo;architecturally determined complexity\u0026rdquo; in runtime/area/energy than both von Neumann architecture and traditional AP paradigm in performing graph computation. We also provide detailed micro-architectures and circuits to best implement the proposed computing model, with domain-special language support. A preliminary published version of 2D AP [Khoram2018CAL] was recognized as best of CAL (IEEE Computer Architecture Letters) in 2018.\n","date":1565989210,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989210,"objectID":"6ecb5c779e78bbdfcc3e4e6c9dd9a670","permalink":"https://penn-cil.github.io/project/two-d-ap/","publishdate":"2019-08-16T16:00:10-05:00","relpermalink":"/project/two-d-ap/","section":"project","summary":"A radically new computing paradigm to further advance FPGA-based graph processing architectures and fundamentally address their limitations.","tags":["graph","associative processing","general purpose computing"],"title":"Two Dimensional Associative Processor (2D AP)","type":"project"},{"authors":["Jialiang Zhang"],"categories":[],"content":"Hardware-software co-design studies targeting next-generation computer systems that are equipped with emerging memories (HBM/HMC, NVM, etc.) are fundamentally hampered by a lack of scalable, performant and accurate simulation platform. Using software simulator is fundamentally bottlenecked by the low simulation speed and low fidelity, making it impractical to run realistic software stack. Such a challenge has limited effective cross-stack innovations (across computer architecture, OS, compiler, machine learning, and domain sciences). Past efforts have largely focused on simulation infrastructure for processor cores with highly simplified models of the memory subsystem. As a key contribution to the community (including SRC JUMP centers, broad academia and industry) to better drive cross-stack memory system research, we have been developing MEG [Zhang2019FCCM] – an open source FPGA- based simulation platform that enables cycle-exact micro-architecture simulation for computer systems with a special focus on memory subsystem (heterogeneous memory, near/in-memory acceleration). In MEG, we combine silicon- proven RTL design of RISC-V cores with configurable heterogeneous memory subsystems (HMC/HBM/NVM). It is capable of running realistic software stacks including booting Linux and comprehensive application software with high fidelity (cycle-exact microarchitectural models derived from synthesizable RTL), flexibility (modifiable to include custom RTL user IP and/or more abstract models), reproducibility, observability, target software support and performance. MEG can be an effective complement to previous RAMP and more recent Firesim projects in covering memory space and a contribution to the open source RISC-V community. ","date":1565989128,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989128,"objectID":"e3ba9260fb5ec7167f3bb7e19fd37b8e","permalink":"https://penn-cil.github.io/project/meg/","publishdate":"2019-08-16T15:58:48-05:00","relpermalink":"/project/meg/","section":"project","summary":"An open source FPGA-based simulation platform for computer systems","tags":["open source","FPGA","nonvolatile memory","RISC-V","in-memory","near-memory"],"title":"MEG","type":"project"},{"authors":["Yue Zha"],"categories":[],"content":"While traditionally being used in embedded systems, custom silicon (e.g., FPGAs) has recently begun to make their way into data centers and the cloud (Amazon AWS EC2 F1 instances, Microsoft Brainwave, Google TPU etc.). While these programmable data-flow architectures provide the lucrative benefits of fine-grained parallelism and high flexibility to accelerate a wide spectrum of applications, system support for them in the context of multi-tenant cloud environment, however, is in its infancy and has two major limitations, 1) inefficient resource management due to the tight coupling between compilation and runtime management, and 2) high programming complexity when exploiting scale-out acceleration, for which the root cause is that hardware resources are not virtualized.\nIn this research, we take FPGA as a case study and develop a full stack solution that can address these limitations and thus, enable virtualization of FPGA clusters in multi-tenant cloud computing environment. Specifically, the key contribution is a new system abstraction that can effectively decouple the compilation and runtime resource management. It allows applications to be compiled offline onto the proposed abstraction and resource allocation to be dynamically determined at runtime. Moreover, it creates an illusion of a single/large virtual FPGA to users, thereby reducing the programming complexity and supporting scale-out acceleration. It also provides virtualization support for the peripheral components (e.g. on-board DRAM, Ethernet), as well as protection and isolation support to ensure a secure execution in multi-tenant cloud environment [Zha2020ASPLOS].\n","date":1565989114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989114,"objectID":"c9127e5962fa60125ba5ae50ebdc7f80","permalink":"https://penn-cil.github.io/project/virtualization/","publishdate":"2019-08-16T15:58:34-05:00","relpermalink":"/project/virtualization/","section":"project","summary":"Virtualization of FPGA clusters in multi-tenant cloud computing environment","tags":["virtualization","cloud","multi-tenant","FPGA"],"title":"Virtualization in Cloud","type":"project"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"Extremely large, sparse graphs with billions of nodes and hundreds of billions of edges arise in many important problem domains ranging from social science, bioinformatics, to video content analysis and search engines. In response to the increasingly larger and more diverse graphs, and the critical need of analyzing them, we focus on large scale graph analytics, an essential class of big data analysis, to explore the comprehensive relationship among a vast collection of interconnected entities. However, it is challenging for existing computer systems to process the massive-scale real-world graphs, not only due to their large memory footprint, but also that most graph algorithms entail irregular memory access patterns and a low compute-to-memory access ratio.\nIn this research, we invented \u0026ldquo;degree-aware\u0026rdquo; hardware/software techniques to improve graph processing efficiency. Our research is built atop a key insight that we obtained from architecture-independent algorithm analysis, which has not been revealed in prior work. More specifically, we identified that a key challenge in processing massive-scale graphs is the redundant graph computations caused by the presence of high-degree vertices which not only increase the total amount of computations but also incur unnecessary random data access. To address this challenge, we developed variants of graph processing systems on an FPGA-HMC platform [Zhang2018FPGA-Graph, Khoram2018FPGA, Zhang2017FPGA-BFS]. For the first time, we leverage the inherent graph property i.e. vertex degree to co-optimize algorithm and hardware architecture. In particular, the unique contributions we made include two algorithm optimization techniques: degree-aware adjacency list reordering and degree-aware vertex index sorting. The former reduces the number of redundant graph computations, while the latter creates a strong correlation between vertex index and data access frequency, which can be effectively applied to guide the hardware design. Further, by leveraging the strong correlation between vertex index and data access frequency created by degree-aware vertex index sorting, we developed two platform-dependent hardware optimization techniques, namely degree-aware data placement and degree-aware adjacency list compression. These two techniques together substantially reduce the amount of external memory access. Finally, we completed the full system design on an FPGA-HMC platform to verify the effectiveness of these techniques. Our implementation achieved the highest performance (45.8 billion traversed edges per second) among existing FPGA-based graph processing systems and was ranked No. 1 on GreenGraph500 list. Green Graph500 (updated June 19, 2019)\n","date":1565989326,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989326,"objectID":"d664d5bbd30ff89c03d183f4b6ec5af9","permalink":"https://penn-cil.github.io/project/large-graph/","publishdate":"2019-08-16T16:02:06-05:00","relpermalink":"/project/large-graph/","section":"project","summary":"*degree-aware* hardware/software techniques to improve graph processing efficiency.","tags":["graph","graph analytics","FPGA-HMC","BFS"],"title":"Large Scale Graph Analytics","type":"project"},{"authors":["Jialiang Zhang"],"categories":[],"content":"Deep neural networks (DNNs) deliver impressive results for a variety of challenging tasks in computer vision, speech recognition, and natural language processing, at the cost of higher computational complexity and larger model size. To reduce the load of taxing DNN infrastructures, a number of FPGA-based DNN accelerators have been proposed via new micro-architectures, data ow optimizations, or algorithmic transformation. Due to the extremely large design space, it is challenging to attain good insights on how to design optimal accelerators on a target FPGA.\nIn this research, we take an alternative, and more principled approach to guide accelerator architecture design and optimization. We borrow the insights from the roofline model and further improve it by taking both on-chip and off-chip memory bandwidth into consideration. We apply the model to quantify the difference between available resources provided by native hardware (FPGA devices) and actual resources demanded by the application (CNN classification kernel). To tackle the problem, we develop a number of hardware/software techniques and implement them on FPGA [Zhang2017FPGA-CNN]. The demonstrated accelerator was recognized as the highest performance and the most energy efficient accelerator for dense convolutional neural network (CNN) compared to the state-of-the-art FPGA-based designs.\n","date":1565989071,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565989071,"objectID":"cbe0c24c174e9c23efd27e59e7e56acc","permalink":"https://penn-cil.github.io/project/deep-learning/","publishdate":"2019-08-16T15:57:51-05:00","relpermalink":"/project/deep-learning/","section":"project","summary":"An *alternative, and more principled approach* to guide accelerator architecture design and optimization","tags":["deep learning","DNN","AI","FPGA","roofline model","convolutional neural network","CNN"],"title":"Deep Learning and AI","type":"project"},{"authors":["Jing Li"],"categories":[],"content":"Honored to join the round table discussion on SRC-SIA Webinar Decadal Plan for Semiconductors: New Trajectories for Memory and Storage, with Dr. Sean Eilert (Micron), Dr. Carolyn Duran (Intel), Dr. David Pellerin (Amazon), Dr. Steffen Hellmold (Twist Bioscience), Dr. Jesse Mee, moderated by Dr. Heike Riel (IBM). Please check out the Webinar recording\n","date":1639068276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639068276,"objectID":"26d5b61f3600868c96dc1cbacb093a0d","permalink":"https://penn-cil.github.io/post/src-sia-webnar/","publishdate":"2021-12-09T11:44:36-05:00","relpermalink":"/post/src-sia-webnar/","section":"post","summary":"With Dr. Sean Eilert (Micron), Dr. Carolyn Duran (Intel), Dr. David Pellerin (Amazon), Dr. Steffen Hellmold (Twist Bioscience), Dr. Jesse Mee, moderated by Dr. Heike Riel (IBM). ","tags":[],"title":"Honored to join the Roundtable SRC-SIA Webinar Decadal Plan for Semiconductors: New Trajectories for Memory and Storage","type":"post"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"ENIAD is on Penn Today!\nMore news can be found at Penn Engineering Today and TCFPGA News.\n","date":1628700276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628700276,"objectID":"3d27aa233b1e8a6a5dd89d78a9085759","permalink":"https://penn-cil.github.io/post/penntoday2021/","publishdate":"2021-08-11T11:44:36-05:00","relpermalink":"/post/penntoday2021/","section":"post","summary":"Penn Engineering’s ENIAD sets new world record for energy-efficient supercomputing","tags":["graph"],"title":"ENIAD is on PennToday!","type":"post"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"We have just broken another world record with our ENIAD design. For this year\u0026rsquo;s Graph500, ENIAD is the top-ranking supercomputer on the latest GreenGraph500 list. We have broken our previous record that we made at UW-madison in 2019 and created a new world record at Penn!\nSupercomputers are the fastest high-performance computer systems available at any given time, allowing the large government or research organizations to tackle grand challenges (e.g., drug discovery for COVID-19, global climate simulation) that would be impossible with regular computers. Graph500 is the de facto standard to measure and quantitatively rank the performance and energy efficiency of supercomputers worldwide for running large graphs using a performance metric called MTEPS/W which represents million traversed edges per second per watt. More specifically, Green Graph500 is a list of the most energy efficient supercomputers in the world ranked by their energy efficiency from running the provided graph analytics applications. ENIAD achieved 6028.85 MTEPS/W, which outperformed \u0026ldquo;Tianhe-3\u0026rdquo; (4724.30 MTEPS/W) - the latest supercomputer in China and IBM supercomputer \u0026ldquo;Minsky\u0026rdquo; (1165.71 MTEPS/W).\nThe Graph500 competition is one important showcase for ENIAD but its capability extends to AI-enriched data analytics in general (e.g., cognitive search).\nMore news can be found at Penn Engineering Today and TCFPGA News.\n","date":1625503476,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625503476,"objectID":"c4165e90898846314e30caa87aea3d83","permalink":"https://penn-cil.github.io/post/greengraph500-2021/","publishdate":"2021-07-05T11:44:36-05:00","relpermalink":"/post/greengraph500-2021/","section":"post","summary":"Our ENIAD is ranked No. 1 on the latest GreenGraph500 list. Congrats to Jialiang!","tags":["graph"],"title":"Our supercomputer ranked No. 1 in the world!","type":"post"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"We demostrated ENIAD – the first end-to-end computing infrastructure for future AI-enriched Big Data serving in real time at cloud scale. AI-enriched Big Data analytics is a fundamental technology to enable more powerful and intelligent cloud services such as cognitive search, as it has built-in AI capability that not only searches keywords but also can uncover contextual insights from ALL types of data sources (text, image, audio and video). However, the required large data volume, diverse indexing structure and complex dynamic data management pose severe challenges on contemporary datacenter infrastructures. ENIAD, named after ENIAC, was architected to address these critical challenges through a series of hardware and software innovations including near-data computation, reconfigurable computing and rapid/agile hardware deployment flow.\nAt Penn, we have a long history in leading computing research. This year, we celebrated the 75th anniversary of ENIAC. ENIAD, as a salute to ENIAC, achieved the world-record performance compared to state-of-the-art CPU and GPU-based data center infrastructures. ENIAD will be announced at this year’s Hot Chips(August 22 - 24, 2021).\n","date":1625182122,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625182122,"objectID":"f372144ad6d3105468474fd2d4b81bea","permalink":"https://penn-cil.github.io/post/hotchips2021/","publishdate":"2021-07-01T19:28:42-04:00","relpermalink":"/post/hotchips2021/","section":"post","summary":"We demostrated ENIAD - the first system for future AI-enriched Big Data serving at cloud scale. Congrats to Jialiang!","tags":["Deep Leaning and AI","large-graph"],"title":"Our ENIAD (a successor to ENIAC) will be announced at Hotchips 2021","type":"post"},{"authors":["Jing Li"],"categories":[],"content":"http://www.nsf-pim.com/schedule.html\n","date":1616365369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616365369,"objectID":"412f84ebffda35d5bd60b7f6c7ac2d42","permalink":"https://penn-cil.github.io/post/nsf_pim/","publishdate":"2021-03-21T18:22:49-04:00","relpermalink":"/post/nsf_pim/","section":"post","summary":"With Prof. Kevin Skadron (UVA), Prof. Jose Martinez (Cornell), Dr. Mike Coyle (Army Research Office), Prof. Rajeev Balasubramonian (Utah)","tags":["service"],"title":"Prof. Li served on the Architecture panel of NSF PIM workshop moderated by Prof. Yiran Chen (Duke)","type":"post"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"274baf84c4a28e59fcc99f8762a4c438","permalink":"https://penn-cil.github.io/publication/zha-2021-vital-isa/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/publication/zha-2021-vital-isa/","section":"publication","summary":"The paper is the first to propose a new virtualization mechanism that can natively support scale-out acceleration across **heterogeneous** cloud FPGAs and demonstrate its effectiveness on a custom FPGA cluster with heterogeneous FPGA resources. Cloud FPGA virtualization mainly targets homogeneous FPGAs in the previous works. The key contribution of this work is to introduce a new system abstraction that can serve as an indirection layer to bridge high-level Application-specific (AS) ISA  and low-level Hardware-specific (HS) abstraction. A set of custom tools is developed to implement the proposed virtualization mechanism with high mapping quality. Additional optimization technique and system support are also provided to maximally hide the inter-FPGA communication latency when exploiting scale-out acceleration using multiple heterogeneous FPGAs.","tags":["conference","virtualization","field-programmable gate array","FPGA","Heterogeneous"],"title":"When Application-Specific ISA Meets FPGAs: A Multi-LayerVirtualization Framework for Heterogeneous Cloud FPGAs","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"Yue Zha and Jing Li, \u0026ldquo;Hetero-ViTAL: A Virtualization Stack for Heterogeneous FPGA Clusters\u0026quot;, in the 48th IEEE/ACM International Symposium on Computer Architecture (ISCA \u0026lsquo;21).\n","date":1614898185,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614898185,"objectID":"5854a4b31e950640dbfcbd447df4f211","permalink":"https://penn-cil.github.io/post/isca_heterovital/","publishdate":"2021-03-04T18:49:45-04:00","relpermalink":"/post/isca_heterovital/","section":"post","summary":"Our paper on virtualizing heterogeneous cloud FPGAs got accepted at ISCA 2021. Congrats to Yue!","tags":["Cloud FPGA virtualization","Heterogeneous"],"title":"Our paper got accepted at ISCA'21","type":"post"},{"authors":["Jing Li"],"categories":[],"content":"https://www.iscaconf.org/isca2021/submit/industry.php\n","date":1611617322,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611617322,"objectID":"75643603a47148d8fa26829a600cc636","permalink":"https://penn-cil.github.io/post/isca_industry_track-2021/","publishdate":"2021-01-25T19:28:42-04:00","relpermalink":"/post/isca_industry_track-2021/","section":"post","summary":"Proud to be on the program committee for ISCA'21 industry track with Prof. David Patterson and Prof. Mark Hill.","tags":["service"],"title":"Prof. Li served as PC for ISCA'21 industry track chaired by Dr. Sean Lee (Facebook)","type":"post"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"Yue Zha and Jing Li, \u0026ldquo;When Application-Specific ISA Meets FPGAs: A Multi-LayerVirtualization Framework for Heterogeneous Cloud FPGAs\u0026quot;, in the 25th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u0026lsquo;21).\n","date":1605828522,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605828522,"objectID":"3525e8dceb1493775b6dfcc90e83b4a5","permalink":"https://penn-cil.github.io/post/asplos2021/","publishdate":"2020-11-19T19:28:42-04:00","relpermalink":"/post/asplos2021/","section":"post","summary":"Our paper on a multi-layer virtualization framework for cloud FPGAs got accepted at ASPLOS 2021. Congrats to Yue!","tags":["Cloud FPGA virtualization","Heterogeneous"],"title":"Our paper got accepted at ASPLOS'21","type":"post"},{"authors":["Jing Li"],"categories":[],"content":"Please refer to the details.\n","date":1595529923,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595529923,"objectID":"8dee13d355b5f77e4571526bddeebc62","permalink":"https://penn-cil.github.io/post/phd-openings-available/","publishdate":"2020-07-23T14:45:23-04:00","relpermalink":"/post/phd-openings-available/","section":"post","summary":"We have multiple PhD openings available.","tags":["openings"],"title":"PhD Openings Available","type":"post"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587254400,"objectID":"3895df0e64188904d8c6fd8e6187c866","permalink":"https://penn-cil.github.io/publication/zha-2020-hyper-ap/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/publication/zha-2020-hyper-ap/","section":"publication","summary":"3D-stacking memory technology such as  High-Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) provides orders of magnitude more bandwidth and significantly increased channel-level parallelism (CLP) due to the new parallel memory architecture. However, it is challenging to fully exploit the abundant CLP for performance as the bandwidth utilization is highly dependent on address mapping in the memory controller. Unfortunately, CLP is very sensitive to a program's data access pattern, which is not made available to OS/hardware by existing mechanisms. . In this work, we address these challenges with software-defined address mapping. We first apply machine learning to learn/predict the program's access patterns and then use clustering to distinguish between multiple patterns in a single program. We provide mechanisms to communicate the learned program's access properties to the OS and hardware and to use it to control data placement in hardware. To guarantee correctness and reduce overhead in storage and performance, we extend Linux kernel and c-language memory allocators to support multiple address mappings. We demonstrate the benefits of our design on real system prototype, comprising (1) a RISC-V processor and HBM modules using Xilinx FPGA platform (2) a bootable OS based on Linux and glibc. Our evaluation on both a CPU and a near-memory accelerator demonstrates a 1.42x and 2.25x speedup in our system with software-defined address mapping compared to a baseline system that uses a fixed address mapping.","tags":["conference","associative processing"],"title":"Hyper-AP: Enhancing Associative Processing Through A Full-Stack Optimization","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"Yue Zha and Jing Li, \u0026ldquo;Hyper-AP: Enhancing Associative Processing Through A Full-Stack Optimization\u0026quot;, IEEE/ACM International Symposium on Computer Architecture (ISCA), 2020.\n","date":1586474922,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586474922,"objectID":"cd39229821813f6149c2632ba01c671a","permalink":"https://penn-cil.github.io/post/isca2020/","publishdate":"2020-04-09T19:28:42-04:00","relpermalink":"/post/isca2020/","section":"post","summary":"Our paper on a new associative processor with full-stack optimization got accepted at ISCA. Congrats to Yue!","tags":["associative processing"],"title":"Our paper got accepted at ISCA'20","type":"post"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1584316800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584316800,"objectID":"34c714367b06089c8557feb35f61dadb","permalink":"https://penn-cil.github.io/publication/zha-2020-asplos/","publishdate":"2020-03-16T00:00:00Z","relpermalink":"/publication/zha-2020-asplos/","section":"publication","summary":"Field-Programmable Gate Arrays (FPGAs) have been integrated into the cloud infrastructure to enhance its computing performance by supporting on-demand acceleration. However, system support for FPGAs in the context of the cloud environment is still in its infancy with two major limitations, i.e., the inefficient runtime management due to the tight coupling between compilation and resource allocation, and the high programming complexity when exploiting scale-out acceleration. The root cause is that FPGA resources are not virtualized. In this paper, we propose a full-stack solution, namely ViTAL, to address the aforementioned limitations by virtualizing FPGA resources. Specifically, ViTAL provides a homogeneous abstraction to decouple the compilation and resource allocation. Applications are offline compiled onto the abstraction, while the resource allocation is dynamically determined at runtime. Enabled by a latency-insensitive communication interface, applications can be mapped flexibly onto either one FPGA or multiple FPGAs to maximize the resource utilization and the aggregated system throughput. Meanwhile, ViTAL creates an illusion of a single and large FPGA to users, thereby reducing the programming complexity and supporting scale-out acceleration. Moreover, ViTAL also provides virtualization support for peripheral components (e.g., on-board DRAM and Ethernet), as well as protection and isolation support to ensure a secure execution in the multi-user cloud environment. We evaluate ViTAL on a real system - an FPGA cluster composed of the latest Xilinx UltraScale+ FPGAs (XCVU37P). The results show that, compared with the existing management method, ViTAL enables fine-grained resource sharing and reduces the response time by 82% on average (improving Quality-of-Service) with a marginal virtualization overhead. Moreover, ViTAL also reduces the response time by 25% compared to AmorphOS (operating in high-throughput mode), a recently proposed FPGA virtualization method.","tags":["conference","virtualization","field-programmable gate array","FPGA"],"title":"ViTAL: Virtualizing FPGAs in the Cloud","type":"publication"},{"authors":["Yue Zha","Etienne Nowak","Jing Li"],"categories":[],"content":"","date":1578873600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578873600,"objectID":"2629d4e225094fae947f2a5e2809e022","permalink":"https://penn-cil.github.io/publication/zha-2020-jssc/","publishdate":"2020-01-13T00:00:00Z","relpermalink":"/publication/zha-2020-jssc/","section":"publication","summary":"The slowdown of the CMOS technology scaling, and the trade-off between efficiency and flexibility have fueled the exploration into novel architectures with emerging post-CMOS technology e.g., resistive-RAM (RRAM). In this article, a nonvolatile fully programmable processing-in-memory (PIM) processor named Liquid Silicon is demonstrated, which combines the superior programmability of general-purpose computing devices e.g., field-programmable gate array (FPGA) and the high efficiency of domain-specific accelerators. Besides the general computing applications, Liquid Silicon is particularly well suited for artificial intelligence (AI)/machine learning and big data applications, which not only poses high computational/memory demand but also evolves rapidly. To fabricate the Liquid Silicon chip, the HfO 2 RRAM is monolithically integrated on top of the commercial 130 nm CMOS. Our measurement confirms that Liquid Silicon chip can operate reliably at a low voltage of 650 mV. It achieves 60.9 TOPS/W in performing neural network (NN) inferences, and 480 GOPS/W in performing content-based similarity search (a key big data application) at a nominal voltage supply of 1.2 V, showing 3x and 100x improvement over the state-of-the-art domain-specific CMOS-/RRAM-based accelerators. In addition, it outperforms the latest nonvolatile FPGA in energy efficiency by 3x in general computing applications.","tags":["journal","liquid silicon","In-memory processing","neural network (NN)","nonvolatile","reconfigurable architecture","resistive-RAM (RRAM)","ternary content-addressable memory"],"title":"Liquid Silicon: A Nonvolatile Fully Programmable Processing-In-Memory Processor with Monolithically Integrated ReRAM for Big Data/Machine Learning Applications (INVITED)","type":"publication"},{"authors":["Yue Zha","Etienne Nowak","Jing Li"],"categories":[],"content":"Yue Zha, Etienne Nowak, and Jing Li, \u0026ldquo;Liquid Silicon: A Nonvolatile Fully Programmable Processing-In-Memory Processor with Monolithically Integrated ReRAM for Big Data/Machine Learning Applications (invited)\u0026quot;, IEEE Journal of Solid-State Circuits (JSSC),2020.\n","date":1577249295,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577249295,"objectID":"f72db39a1d984afcd32cb3cd80e1bace","permalink":"https://penn-cil.github.io/post/liquid-silicon-jssc-2020/","publishdate":"2019-12-24T22:48:15-06:00","relpermalink":"/post/liquid-silicon-jssc-2020/","section":"post","summary":"Our invited paper on Liquid Silicon got accepted at JSSC. Congrats to all the authors!","tags":["liquid silicon"],"title":"Our invited paper got accepted at JSSC","type":"post"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"Yue Zha and Jing Li, \u0026ldquo;ViTAL: Virtualizing FPGAs in the Cloud\u0026quot;, in the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS \u0026lsquo;20).\n","date":1577249295,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577249295,"objectID":"0e4d7b9e8844ab4377844c4b9af5ad8e","permalink":"https://penn-cil.github.io/post/fpga-virtualization-asplos-2020/","publishdate":"2019-12-24T22:48:15-06:00","relpermalink":"/post/fpga-virtualization-asplos-2020/","section":"post","summary":"Our paper on FPGA virtualization got accepted at ASPLOS 2020. Congrats to Yue!","tags":["FPGA","virtualization"],"title":"Our paper got accepted at ASPLOS 2020","type":"post"},{"authors":["Soroosh Khoram","Stephen J Wright","Jing Li"],"categories":[],"content":"","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"fdc5893b56bfa639502b27610bf7e1f6","permalink":"https://penn-cil.github.io/publication/khoram-2019-interleaved/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/publication/khoram-2019-interleaved/","section":"publication","summary":"Similarity search retrieves the nearest neighbors of a query vector from a dataset of high-dimensional vectors. As the size of the dataset grows, the cost of performing the distance computations needed to implement a query can become prohibitive. A method often used to reduce this computational cost is quantization of the vector space and location-based encoding of the dataset vectors. These encodings can be used during query processing to find approximate nearest neighbors of the query point quickly. Search speed can be improved by using shorter codes, but shorter codes have higher quantization error, leading to degraded precision. In this work, we propose the Interleaved Composite Quantization (ICQ) which achieves fast similarity search without using shorter codes. In ICQ, a small subset of the code is used to approximate the distances, with complete codes being used only when necessary. Our method effectively reduces both code length and quantization error. Furthermore, ICQ is compatible with several recently proposed techniques for reducing quantization error and can be used in conjunction with these other techniques to improve results. We confirm these claims and show strong empirical performance of ICQ using several synthetic and real-word datasets.","tags":["whitepaper"],"title":"Interleaved Composite Quantization for High-Dimensional Similarity Search","type":"publication"},{"authors":["Soroosh Khoram","Jing Li"],"categories":[],"content":"","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"49a66d685cb369bf1d4fee88a4c55c72","permalink":"https://penn-cil.github.io/publication/khoram-2019-toco/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/publication/khoram-2019-toco/","section":"publication","summary":"Neural network compression methods have enabled deploying large models on emerging edge devices with little cost, by adapting already-trained models to the constraints of these devices. The rapid development of AI-capable edge devices with limited computation and storage requires streamlined methodologies that can efficiently satisfy the constraints of different devices. In contrast, existing methods often rely on heuristic and manual adjustments to maintain accuracy, support only coarse compression policies, or target specific device constraints that limit their applicability. We address these limitations by proposing the TOlerance-based COmpression (TOCO) framework. TOCO uses an in-depth analysis of the model, to maintain the accuracy, in an active learning system. The results of the analysis are tolerances that can be used to perform compression in a fine-grained manner. Finally, by decoupling compression from the tolerance analysis, TOCO allows flexibility to changes in the hardware.","tags":["whitepaper"],"title":"TOCO: A Framework for Compressing Neural Network Models Based on Tolerance Analysis","type":"publication"},{"authors":null,"categories":null,"content":"Twenty-Eighth ACM/SIGDA International Symposium on Field-Programmable Gate Arrays\nhttp://www.isfpga.org\nFebruary 24-26, 2020\nEmbassy Suites by Hilton Monterey Bay Seaside\n1441 Canyon Del Rey, Seaside, California, 93955, USA\nAbstract Submission Deadline: September 9, 2019\nFull paper Submission Deadline: September 9, 2019\nNEW Artifact Submission Deadline: September 9, 2019\\\nThe ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA 2020) is the premier conference for presentation of advances in FPGA technology. Accepted papers will be published in the conference proceedings and available in the ACM Digital Library.\nTypes of Submissions Sought 1. Research Papers (with and without Artifacts) As usual, we solicit research papers related to the following areas:\n FPGA Architecture: Architectures for programmable logic fabrics or their components, including routing, flexible logic cells, embedded blocks (memory, DSP, processors), and I/O interfaces. Novel commercial architectures and architectural features. FPGA Circuit Design: Circuits and layout techniques for the design of FPGAs. Impact of future process and design technologies on FPGAs as well as novel memory memory or nano-scale devices. Methods for analyzing and improving static and dynamic power, power and clock distribution, yield, manufacturability, security, reliability, and testability. CAD for FPGAs: Algorithms for synthesis, technology mapping, logic and timing optimization, clustering, placement, and routing of FPGAs. Novel design software for system-level partitioning, debug, and verification. Algorithms for modeling, analysis and optimization of timing and power. High-Level Abstractions and Tools for FPGAs: General-purpose and domain-specific languages, tools, and techniques to facilitate the design, debugging and verification of FPGA-based applications and systems. Novel hardware/software co-design and high-level synthesis methodologies enabling digital signal processing, compute acceleration, networking, machine learning, and embedded systems. FPGA-based and FPGA-like Computing Engines: Systems and software for compiled accelerators, reconfigurable/adaptive computing, and rapid-prototyping. Programmable overlay architectures implemented using FPGAs. Applications and Design Studies: Implementation of novel designs on FPGAs establishing state-of-the-art in high-performance, low-power, security, or high-reliability. Designs leveraging unique capabilities of FPGA architectures or demonstrating significant improvements over alternative programmable technologies (e.g., CPU, GPU). Design studies or architecture explorations enabling improvement of FPGA architectures.  Research submissions may be either:\n Full: at most 10 pages (excluding references), for a full presentation at the conference; or Short: at most 6 pages (excluding references), for a brief presentation.  A paper submitted as either full or short will only be considered in that category and may include artifacts if desired (see below for more details on artifact submission and evaluation).\n2. Tutorial Papers on Emerging Applications / Methodologies The conference will include a Sunday workshop oriented toward users of FPGAs: be it deep learning implementations, computer security or other emerging topics of interest. For this category, we solicit tutorial papers describing effective design techniques and design flows. The ideal submission will enable beginning researchers to enter the area, current researchers to broaden their scope, and practitioners to gain new insights and applicable skills. Tutorial submissions need not present novel research results, but should integrate expert practical and/or research knowledge related to FPGAs for a broader audience. This may include:\n Technical descriptions of new commercial or academic design tools of general interest; Insightful summaries of the state-of-the-art that suggest open research problems; and In-depth design tutorials and design experiences.  Tutorial submissions are at least 4 and at most 10 pages. Accepted submissions are published in the proceedings and allocated a presentation time of up to one hour, appropriate to the content.\n3. Panel Discussion Proposals We also solicit proposals for the panel discussion at the conference banquet. The submission should outline the topic, questions to be addressed, and suggested speakers.\nSubmission Process - please read carefully Submissions of all types should be made in the form of an English language PDF file, on-line at https://www.softconf.com/j/fpga2020/. Papers should use the sigconf ACM format template posted at http://www.acm.org/publications/proceedings-template/. LaTeX users should use the format used in the sample-sigconf.pdf file under the Samples folder of the zipped master file (downloadable through the LATEX (Version 1.61) link). Microsoft Word users can download the file Interim layout.docx under the Word Authors section of the page. Abstract and Full Paper Submissions must be received by September 9, 2019 at 11:59 PM AoE (Anywhere-on-Earth time zone).\nSubmissions will be considered for acceptance as full or short regular papers, workshop papers, or posters. Regular submissions related to the workshop topic may be scheduled for presentation during the workshop. Regular or workshop submissions will also be considered for acceptance as a poster. A paper submitted to the short or full paper category will only be considered in that category. Once a paper has been submitted, its authorship list is considered to be fixed and finalized. As the inclusion and evaluation of artifacts is new for FPGA 2020, additional information will be provided at http://www.isfpga.org/artifactEvaluation.html by August 2019.\nDouble Blind Policy: The FPGA Symposium uses a double-blind reviewing system. Manuscripts must not identify authors or their affiliations; those that do will not be considered. References to the authors’ prior work should be made in the 3rd person, in the same way one would reference work by others. If necessary to maintain anonymity, citations may be shown as \u0026ldquo;Removed for blind review,\u0026rdquo; but consider that this may impede a thorough review if the removed citation is crucial to understanding the submission. When necessary, authors should cite widely-available Open Source software website(s) without claiming ownership. Grant numbers and other government markings should also be blinded during the review process. Placing a preliminary version of the unpublished paper on arXiv is not disqualifying, but it is also not encouraged. Similarly, if a paper can be unblinded by active search, this is not considered to undermine the spirit of the double-blind review. However, there are resources to blind open-source repositories for review, including: https://github.com/tdurieux/anonymous_github.) If you have questions about how to meet these guidelines, please contact the program chair before the submission deadline.\nReviewer Conflict Policy: During paper submission, all author(s) conflicts must be registered with all possible program committee members. Conflicts are defined as all relationships that would prevent a reviewer from objectively evaluating the submitted work. This includes, but is not limited to, having within the past 5 years: 1) co-authored a publication, 2) shared a funding award, and 3) shared at least one institutional affiliation. Note: if a conflict is declared (or left undeclared) in an attempt to manipulate the review process, the submission may be rejected. In the situation where the potential conflict is with the program chair, please contact the program chair well before the submission deadline.\nOriginality of Submissions: Papers submitted for FPGA 2020 are guaranteed by the authors to be unique manuscripts and not previously published, currently accepted or under consideration for acceptance at another venue. They cannot be substantially similar to any other current/future conference, journal, or workshop submission(s) unless the content appeared at a venue that does not have an archived proceedings.\nRebuttal Process: FPGA 2020 includes a rebuttal phase for authors to provide an optional response of up to 500-words to reviewers’ questions and comments. This information is considered during the final deliberation process. The Rebuttal phase will begin by October 14th and end October 18th.\nAuthor participation: For inclusion in the ACM digital library, at least one of the authors of each accepted submission is required to attend the conference to present the work.\nBest Paper Award and a Special ACM TRETS issue for Best of FPGA 2020 Authors of this year’s best manuscripts will be eligible for the conference\u0026rsquo;s best paper awards. They will also be invited to extend their work for consideration in a special issue of ACM\u0026rsquo;s Transactions on Reconfigurable Technology and Systems (TRETS) for FPGA 2020.\nNEW: Artifact Evaluation FPGA 2020 will allow authors to submit accompanying artifacts for their paper submissions for evaluation. This process will allow ACM recognized badges to be associated the final publication. The inclusion of artifacts with a submission is not required for a paper submission nor will any preference be given to submissions with artifacts over those without. Papers and artifacts will be subjected to separate and independent review processes. Artifact evaluation must NOT interfere with the double blind reviewing process of their accompanying papers, so all accompanying links in the paper to the artifacts should be blinded. All authors will be required at the time of paper submission to indicate if there will be also be associated artifacts for evaluation. If artifacts will be included, a descriptor of their nature will be required as part of the submission. For more information, go to: http://www.isfpga.org/artifactEvaluation.html.\nImportant dates:          Abstract Submissions due: September 9, 2019   Full Paper Submissions due: September 9, 2019   Final Artifacts for Evaluation due: September 9, 2019   Author Paper Rebuttals due: October 18, 2019   Notification of acceptance: Mid-November, 2019   Camera-ready copy of accepted papers due: Early December, 2019    Contact Information For questions about the submission process or technical program:\nLesley Shannon, Program Chair FPGA 2020 ASB 9801, Simon Fraser University 8888 University Dr. Vancouver BC, V5A 1S6, CANADA programchair@isfpga.org For general questions about the conference:\nStephen Neuendorffer, General Chair FPGA 2020 Xilinx 2100 All Programmable Dr. San Jose, CA 95124, USA generalchair@isfpga.org Organizing Committee:\n General Chair: Stephen Neuendorffer, Xilinx, generalchair@isfpga.org . Program Chair: Lesley Shannon, Simon Fraser University, programchair@isfpga.org . Finance Chair: Kia Bazargan, University of Minnesota, financechair@isfpga.org. Artifact Evaluation Co-Chairs: Miriam Leeser, Northeastern University, and Suhaib * Fahmy, University of Warwick, artifactevaluationchair@isfpga.org. Publicity Chair: Jing Li, University of Wisconsin-Madison, publicitychair@isfpga.org.  ","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"83344fde19323cc9756831ff848b5bc7","permalink":"https://penn-cil.github.io/cfp/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/cfp/","section":"","summary":"FPGA 2020 call for paper","tags":null,"title":"Call for Papers - FPGA 2020","type":"page"},{"authors":null,"categories":null,"content":"If you have questions, you can email: artifactevaluationchair@isfpga.org\nThe goal of this initiative is to promote reproducibility of published results by highlighting papers supported with open-source code. These papers are identified by badges promoted by ACM. More details are provided below.\nWe are asking all authors to indicate whether or not there are artifacts associated with their paper(s). Artifacts are not required.\nArtifact Form Requirements What is the Artifact Form? The Artifact Form is part of the information you must fill out when you submit your paper to FPGA 2020. The form describes the presence or absence of computational artifacts (software, hardware or data) that supports the research presented in the paper.\nIs the Artifact Form required in order to submit to FPGA 2020? As an author, you must answer the question whether or not you have artifacts when you submit your paper. If you have artifacts then please complete the remaining questions. Note that you do not need to have artifacts.\nDo I need to make my software open source in order to complete the Artifact Form? No. You are not asked to make any changes to your computing environment or design process in order to complete the form. The form is meant to describe the computing environment in which you produced your results and any artifacts you wish to share. Any author-created software does not need to be open source, unless you wish to be eligible for an ACM Artifacts badge (see below).\nReview Process Who will review my artifact form? The reviewers of your paper will not have access to the Artifact form. If your submitted paper is accepted to FPGA 2020, then the Artifact form will be reviewed. The Artifact Evaluation Committee (AEC) will review the appendices, and will check that artifacts are indeed available at the URLs provided. They will also help authors improve their forms, in a double-open arrangement. If authors select this option (by answering yes to question 12), their paper may be evaluated for the Artifacts Available \u0026ndash; Reusable or Artifacts Available Functional badges. Some papers will be evaluated for the ACM “Results Replicated” badge. If your paper is chosen, then the artifact evaluation committee will be in contact with the authors if they have questions regarding your artifacts.\nHow will review of appendices interact with the double-blind review process? Artifact review will not take place until after decisions on papers have been made. Reviewers will not have access to the artifact form. Authors should not include links to their repositories in their paper. The paper review process is double blind. The artifact review process is not. .\nImpact of Artifact Form (AF) What\u0026rsquo;s the impact of an Artifact Form on scientific reproducibility? Reproducibility depends on, as a first step, sharing the provenance of results with transparency, and the AF is an instrument of documentation and transparency. A good AF helps researchers document their results, and helps other researchers build from them.\nThe paper text explains why I believe my answers are right and shows all my work. Why do I need to provide an AF? There are many good reasons for formalizing the artifact description and evaluation process. Standard practice varies across disciplines. Labeling the evaluation as such improves our ability to review the paper and improves reader confidence in the veracity of the results.\nArtifacts What are \u0026ldquo;author-created\u0026rdquo; artifacts and why make the distinction? Author created artifacts are the hardware, software, or data created by the paper\u0026rsquo;s authors. Only these artifacts need be made available to facilitate reproducibility. Proprietary, closed source artifacts (e.g. commercial software and CPUs) will necessarily be part of many research studies. These proprietary artifacts should be described to the best of the author\u0026rsquo;s ability but do not need to be provided.\nWhat about proprietary author-created artifacts? The ideal case for reproducibility is to have all author-created artifacts publically available with a stable identifier. Papers involving proprietary, closed source author-created artifacts should indicate the availability of the artifacts and describe them as much as possible. Note that results dependent on closed source artifacts are not reproducible and are therefore ineligible for most of the ACM\u0026rsquo;s artifact review badges. See https://www.acm.org/publications/policies/artifact-review-badging.\nAre the numbers used to draw our charts a data artifact? Not necessarily. Data artifacts are the data (input or output) required to reproduce the results, not necessarily the results themselves. For example, if your paper presents a system that generates charts from datasets then providing an input dataset would facilitate reproducibility. However, if the paper merely uses charts to elucidate results then the input data to whatever tool you used to draw those charts isn\u0026rsquo;t required to reproduce the paper\u0026rsquo;s results. The tool which drew the chart isn\u0026rsquo;t part of the study, so the input data to that tool is not a data artifact of this work.\nHelp! My data is HUGE! How do I make it publically available with a stable identifier? Use Zenodo (https://help.zenodo.org/). Contact them for information on how to upload extremely large datasets. You can easily upload datasets of 50GB or less, have multiple datasets, and there is no size limit on communities.\nWhat\u0026rsquo;s the impact of an Artifact Form on scientific reproducibility? An artifact-evaluation effort can increase the trustworthiness of computational results. It can be particularly effective in the case of results obtained using specialized computing platforms, not available to other researchers. Leadership computing platforms, novel testbeds, and experimental computing environments are of keen interest to the FPGA community. Access to these systems is typically limited, however. Thus, most reviewers cannot independently check results, and the authors themselves may be unable to recompute their own results in the future, given the impact of irreversible changes in the environment (compilers, libraries, components, etc.). The various forms of Artifact Evaluation improve confidence that computational results from these special platforms are correct.\nACM Artifacts Available and Artifacts Evaluated Badges For more information, see https://www.acm.org/publications/policies/artifact-review-badging\nThe badges we will consider for FPGA2020 are Artifacts Available, Artifacts Evaluated — Functional, Artifacts Evaluated — Reusable and Results Validated.\nBy the ACM Badging definitions:\n Artifacts Available badge: This badge is applied to papers in which associated artifacts have been made permanently available for retrieval.\n The Artifact form (new for FPGA 2020) will be used to determine eligibility for an ACM Artifacts Available badge on the basis of the answers to questions about the availability of author-created software, hardware or data products. The conditions of eligibility are:\n All author-created software artifacts are maintained in a public repository under an OSI- approved license. All author-created hardware artifacts are available and comply with the Open Source Hardware Definition. All author-created data artifacts are maintained in a public repository with a stable identifier, such as a DOI.   Artifacts Evaluated — Functional badge: The artifacts associated with the research are found to be documented, consistent, complete, exercisable, and include appropriate evidence of verification and validation.\n  Artifacts Evaluated — Reusable badge: The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. That is, they have all the qualities of the Artifacts Evaluated – Functional level, but, in addition, they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to.\n  Results Replicated badge: The main results of the paper have been obtained in a subsequent study by a person or team other than the authors, using, in part, artifacts provided by the author.\n Exact replication or reproduction of results is not required, or even expected. Instead, the results must be in agreement to within a tolerance deemed acceptable for experiments of the given type. In particular, differences in the results should not change the main claims made in the paper.\nIt is easy to see how research articles that develop algorithms or software systems could be labeled as described above. Here, the artifacts could be implementations of algorithms or complete software systems, and replication would involve exercise of software, typically software provided by the author. However, we intend these badges to be applicable to other types of research as well. For example, artifacts associated with human-subject studies of novel human-computer interface modalities might be the collected data, as well as the scripts developed to analyze the data. \u0026ldquo;Replication\u0026rdquo; might focus on a careful inspection of the experimental protocol along with independent analysis of the collected data.\nNotes about artifacts: A design artifact can be a hardware or software design. Software could include design tools or host code. Hardware designs can include designs intended to become ASICs, or designs that map to FPGA fabric.\nHardware artifacts should comply with the Open Source Hardware Definition. See https://www.oshwa.org/definition/\n","date":1565395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565395200,"objectID":"a8c434339c3d68145a9fe9f6890a88f6","permalink":"https://penn-cil.github.io/artifact-evaluation/","publishdate":"2019-08-10T00:00:00Z","relpermalink":"/artifact-evaluation/","section":"","summary":"Guidelines on Artifact Evaluation","tags":null,"title":"Guidelines on Artifact Evaluation","type":"page"},{"authors":null,"categories":null,"content":"Education Work Experience Awards Research Highlights Publications Journal Conference ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"fd36605688ef45e10dc233c860158012","permalink":"https://penn-cil.github.io/cv/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/cv/","section":"","summary":"test on how to add a page to your site.","tags":null,"title":"Jing's CV","type":"page"},{"authors":["Jialiang Zhang"],"categories":[],"content":"Our FPGA-based scalable graph analytics system is ranked No. 1 on the latest GreenGraph500 list.\n","date":1561999476,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561999476,"objectID":"c5031eaf3476760cedc8980eab03ff4a","permalink":"https://penn-cil.github.io/post/greengraph5000/","publishdate":"2019-07-01T11:44:36-05:00","relpermalink":"/post/greengraph5000/","section":"post","summary":"Our FPGA-based scalable graph analytics system is ranked No. 1 on the latest GreenGraph500 list. Congrats to Jialiang!","tags":["graph"],"title":"Our graph analytics system ranked No. 1 on GreenGraph500","type":"post"},{"authors":["Yue Zha","Etienne Nowak","Jing Li"],"categories":[],"content":"","date":1560038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560038400,"objectID":"4ad7d4c346e250a5cbbf231133eb8319","permalink":"https://penn-cil.github.io/publication/zha-2019-vlsic/","publishdate":"2019-06-09T00:00:00Z","relpermalink":"/publication/zha-2019-vlsic/","section":"publication","summary":"A nonvolatile fully programmable processing-in-memory (PIM) processor named Liquid Silicon (L-Si) is demonstrated, which combines the superior programmability of general-purpose computing devices (e.g. FPGA) and the high power efficiency of domain-specific accelerators. Besides the general computing applications, L-Si is particularly well suited for AI/machine learning and big data applications, which not only pose high computational/memory demand but also evolves rapidly. L-Si is fabricated by monolithically integrating HfO 2 resistive RAM on top of commercial 130nm Si CMOS. Our measurement confirmed the fabricated chip operates reliably at low voltage of 650 mV. It achieves 60.9 TOPS/W in performing neural network inferences and 480 GOPS/W in performing content-based similarity search (a key big data application) at nominal voltage supply of 1.2V, showing 3× and ~100× power efficiency improvement over the state-of-the-art domain-specific CMOS-/RRAM-based accelerators. In addition, it outperforms the latest nonvolatile FPGA in energy efficiency by ~3× in general compute-intensive applications.","tags":["conference","liquid silicon"],"title":"Liquid Silicon: A Nonvolatile Fully Programmable Processing-In-Memory Processor with Monolithically Integrated ReRAM for Big Data/Machine Learning Applications","type":"publication"},{"authors":["Qing Luo","Jie Yu","Xumeng Zhang","Kan-Hao Xue","Yan Cheng","Tiancheng Gong","Hangbing Lv","Xiaoxin Xu","Peng Yuan","Jiahao Yin","Lu Tai","Shibing Long","Qi Liu","Jing Li","Ming Liu"],"categories":[],"content":"","date":1560038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560038400,"objectID":"9b6a9545067d4d378508f40d8185bbf8","permalink":"https://penn-cil.github.io/publication/luo-2019-vlsit/","publishdate":"2019-06-09T00:00:00Z","relpermalink":"/publication/luo-2019-vlsit/","section":"publication","summary":"","tags":["conference"],"title":"Nb(1-x)O2 based Universal Selector with Ultra-high Endurance (\u003e10^12), high speed (10ns) and Excellent Vth Stability","type":"publication"},{"authors":["Jialiang Zhang","Yang Liu","Gaurav Jain","Yue Zha","Jonathan Ta","Jing Li"],"categories":[],"content":"","date":1556409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556409600,"objectID":"aab066ed4db87162d423975becd18926","permalink":"https://penn-cil.github.io/publication/zhang-2019-fccm/","publishdate":"2019-04-28T00:00:00Z","relpermalink":"/publication/zhang-2019-fccm/","section":"publication","summary":"Emerging 3D memory technologies, such as the Hybrid Memory Cube (HMC) and High Bandwidth Memory (HBM), provide increased bandwidth and massive memory-level parallelism. Efficiently integrating emerging memories into existing system pose new challenges and require detailed evaluation in a real computing environment. In this paper, we propose MEG, an open-source, configurable, cycle-exact, and RISC-V based full system simulation infrastructure using FPGA and HMC. MEG has three highly configurable design components: (i) a HMC adaptation module that not only enables communication between the HMC device and the processor cores but also can be extended to fit other memories (e.g., HBM, nonvolatile memory) with minimal effort, (ii) a reconfigurable memory controller along with its OS support that can be effectively leveraged by system designers to perform software-hardware co-optimization, and (iii) a performance monitor module that effectively improves the observability and debuggability of the system to guide performance optimization. We provide a prototype implementation of MEG on Xilinx VCU110 board and demonstrate its capability, fidelity, and flexibility on real-world benchmark applications. We hope that our open-source release of MEG fills a gap in the space of publicly-available FPGA-based full system simulation infrastructures specifically targeting memory system and inspires further collaborative software/hardware innovations.","tags":["conference","MEG","RISC-V"],"title":"MEG: A RISCV-based system simulation infrastructure for exploring memory optimization using FPGAs and Hybrid Memory Cube (Best Paper Nominee)","type":"publication"},{"authors":["Jing Li"],"categories":null,"content":"","date":1554332400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554332400,"objectID":"4807f403630f5cd54f6105f9b1de1a5d","permalink":"https://penn-cil.github.io/talk/liquid-silicon-stanford-2019/","publishdate":"2019-08-14T17:26:11-05:00","relpermalink":"/talk/liquid-silicon-stanford-2019/","section":"talk","summary":"","tags":["liquid silicon"],"title":"Liquid Silicon: A 10-year Journey Towards New Computing Paradigm","type":"talk"},{"authors":["Alexander Ratner","Dan Alistarh","Gustavo Alonso","David G. Andersen","Peter Bailis","Sarah Bird","Nicholas Carlini","Bryan Catanzaro","Jennifer Chayes","Eric Chung","Bill Dally","Jeff Dean","Inderjit S. Dhillon","Alexandros Dimakis","Pradeep Dubey","Charles Elkan","Grigori Fursin","Gregory R. Ganger","Lise Getoor","Phillip B. Gibbons","Garth A. Gibson","Joseph E. Gonzalez","Justin Gottschlich","Song Han","Kim Hazelwood","Furong Huang","Martin Jaggi","Kevin Jamieson","Michael I. Jordan","Gauri Joshi","Rania Khalaf","Jason Knight","Jakub Konečný","Tim Kraska","Arun Kumar","Anastasios Kyrillidis","Aparna Lakshmiratan","Jing Li","Samuel Madden","H. Brendan McMahan","Erik Meijer","Ioannis Mitliagkas","Rajat Monga","Derek Murray","Kunle Olukotun","Dimitris Papailiopoulos","Gennady Pekhimenko","Theodoros Rekatsinas","Afshin Rostamizadeh","Christopher Ré","Christopher De Sa","Hanie Sedghi","Siddhartha Sen","Virginia Smith","Alex Smola","Dawn Song","Evan Sparks","Ion Stoica","Vivienne Sze","Madeleine Udell","Joaquin Vanschoren","Shivaram Venkataraman","Rashmi Vinayak","Markus Weimer","Andrew Gordon Wilson","Eric Xing","Matei Zaharia","Ce Zhang","Ameet Talwalkar"],"categories":[],"content":"preprint\n","date":1553817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553817600,"objectID":"24496cbf165c1ca3839e4246bcb5d530","permalink":"https://penn-cil.github.io/publication/ratner-2019-sysml/","publishdate":"2019-03-29T00:00:00Z","relpermalink":"/publication/ratner-2019-sysml/","section":"publication","summary":"Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, SysML, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two.","tags":["whitepaper"],"title":"MLSys: The New Frontier of Machine Learning Systems","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1552859012,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565819012,"objectID":"608ff8da4d23e8d3a2933ae2a3659ba0","permalink":"https://penn-cil.github.io/post/liquid-silicon-vlsic-2019/","publishdate":"2019-03-17T16:43:32-05:00","relpermalink":"/post/liquid-silicon-vlsic-2019/","section":"post","summary":"Our paper got accepted at 2019 Symposium on VLSI Circuits. Congrats to Yue!","tags":["liquid silicon"],"title":"Our paper got accepted at 2019 Symposium on VLSI Circuits","type":"post"},{"authors":["Jialiang Zhang"],"categories":[],"content":"","date":1551650076,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565819676,"objectID":"9105aabc98c118d66101164467da18f9","permalink":"https://penn-cil.github.io/post/meg-fccm-2019/","publishdate":"2019-03-03T16:54:36-05:00","relpermalink":"/post/meg-fccm-2019/","section":"post","summary":"Our RISC-V paper got accepted at FCCM’19 and nominated Best Paper. Congrats to all the authors!","tags":["MEG","RISC-V"],"title":"Our RISC-V paper got accepted at FCCM’19 and nominated Best Paper","type":"post"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"","date":1550966400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550966400,"objectID":"78e61279a94b46b55c608e5030bcc45c","permalink":"https://penn-cil.github.io/publication/zhang-2019-fpga/","publishdate":"2019-02-24T00:00:00Z","relpermalink":"/publication/zhang-2019-fpga/","section":"publication","summary":"","tags":["conference"],"title":"Unleashing the Power of Soft Logic for Convolutional Neural Network Acceleration via Product Quantization (Poster)","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://penn-cil.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Jing Li"],"categories":null,"content":"","date":1541536200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541536200,"objectID":"6d0e25e3e75bc248304d29a99da98f8e","permalink":"https://penn-cil.github.io/talk/liquid-silicon-uchicago-2018/","publishdate":"2019-08-14T17:19:32-05:00","relpermalink":"/talk/liquid-silicon-uchicago-2018/","section":"talk","summary":"","tags":["liquid silicon"],"title":"Liquid Silicon: A New Computing Paradigm Enabled by Monolithic 3D Cross-point Memory","type":"talk"},{"authors":["Jing Li"],"categories":null,"content":"","date":1539631800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539631800,"objectID":"9f2791498884b6a8942641506d5de2e9","permalink":"https://penn-cil.github.io/talk/liquid-silicon-ucla-2018/","publishdate":"2019-08-14T17:14:50-05:00","relpermalink":"/talk/liquid-silicon-ucla-2018/","section":"talk","summary":"","tags":["liquid silicon"],"title":"Liquid Silicon: A New Computing Paradigm Enabled by Monolithic 3D Cross-point Memory","type":"talk"},{"authors":["Jialiang Zhang","Soroosh Khoram","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline29%, 979 out of over 3300)\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"f20a393f9e35aa2be3f7d3fbc38f29df","permalink":"https://penn-cil.github.io/publication/zhang-2018-cvpr/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/zhang-2018-cvpr/","section":"publication","summary":"We present a new method for Product Quantization (PQ) based approximated nearest neighbor search (ANN) in high dimensional spaces. Specifically, we first propose a quantization scheme for the codebook of coarse quantizer, product quantizer, and rotation matrix, to reduce the cost of accessing these codebooks. Our approach also combines a highly parallel k-selection method, which can be fused with the distance calculation to reduce the memory overhead. We implement the proposed method on Intel HARPv2 platform using OpenCL-FPGA. The proposed method significantly outperforms state-of-the-art methods on CPU and GPU for high dimensional nearest neighbor queries on billion-scale datasets in terms of query time and accuracy regardless of the batch size. To our best knowledge, this is the first work to demonstrate FPGA performance superior to CPU and GPU on high-dimensional, large-scale ANN datasets.","tags":["conference"],"title":"Efficient Large-scale Approximate Nearest Neighbor Search on the OpenCL-FPGA","type":"publication"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"","date":1524960000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524960000,"objectID":"5d9b396a1660090f6bf0d8c4ca952e6b","permalink":"https://penn-cil.github.io/publication/zhang-2018-fccm/","publishdate":"2018-04-29T00:00:00Z","relpermalink":"/publication/zhang-2018-fccm/","section":"publication","summary":"","tags":["conference"],"title":"PQ-CNN: Accelerating Product Quantized Convolutional Neural Network (Poster)","type":"publication"},{"authors":["Soroosh Khoram","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline34%, 314 out of 935)\n","date":1522540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522540800,"objectID":"151dbcb3674c129a1a5c2d61be01e32e","permalink":"https://penn-cil.github.io/publication/khoram-2018-iclr/","publishdate":"2018-04-01T00:00:00Z","relpermalink":"/publication/khoram-2018-iclr/","section":"publication","summary":"Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.","tags":["conference"],"title":"Adaptive Quantization of Neural Networks","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline18.2%, 56 out of 307)\n","date":1521417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521417600,"objectID":"f388c0f6f4891a6e46c0e6b96f0fd1f1","permalink":"https://penn-cil.github.io/publication/zha-2018-asplos/","publishdate":"2018-03-19T00:00:00Z","relpermalink":"/publication/zha-2018-asplos/","section":"publication","summary":"With the recent trend of promoting Field-Programmable Gate Arrays (FPGAs) to first-class citizens in accelerating compute-intensive applications in networking, cloud services and artificial intelligence, FPGAs face two major challenges in sustaining competitive advantages in performance and energy efficiency for diverse cloud workloads: 1) limited configuration capability for supporting light-weight computations/on-chip data storage to accelerate emerging search-/data-intensive applications. 2) lack of architectural support to hide reconfiguration overhead for assisting virtualization in a cloud computing environment. In this paper, we propose a reconfigurable memory-oriented computing fabric, namely Liquid Silicon-Monona (L-Si), enabled by emerging nonvolatile memory technology i.e. RRAM, to address these two challenges. Specifically, L-Si addresses the first challenge by virtue of a new architecture comprising a 2D array of physically identical but functionally-configurable building blocks. It, for the first time, extends the configuration capabilities of existing FPGAs from computation to the whole spectrum ranging from computation to data storage. It allows users to better customize hardware by flexibly partitioning hardware resources between computation and memory, greatly benefiting emerging search- and data-intensive applications. To address the second challenge, L-Si provides scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization. In addition, we provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. Our evaluation results show L-Si achieves 99.6% area reduction, 1.43× throughput improvement and 94.0% power reduction on search-intensive benchmarks, as compared with the FPGA baseline. For neural network benchmarks, on average, L-Si achieves 52.3× speedup, 113.9× energy reduction and 81% area reduction over the FPGA baseline. In addition, the multi-context architecture of L-Si reduces the context switching time to - 10ns, compared with an off-the-shelf FPGA (∼100ms), greatly facilitating virtualization.","tags":["liquid silicon","processing-in-memory"],"title":"Liquid Silicon-Monona: A Reconfigurable Memory-Oriented Computing Fabric with Scalable Multi-Context Support","type":"publication"},{"authors":["Jing Li"],"categories":[],"content":"","date":1520812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520812800,"objectID":"61be238663e9bcf4936508a1d84128ca","permalink":"https://penn-cil.github.io/publication/li-2018-cstic/","publishdate":"2018-03-12T00:00:00Z","relpermalink":"/publication/li-2018-cstic/","section":"publication","summary":"","tags":["conference"],"title":"Nonvolatile Memory Outlook: Technology Driven or Application Driven? (INVITED)","type":"publication"},{"authors":["Soroosh Khoram","Jialiang Zhang","Maxwell Strange","Jing Li"],"categories":[],"content":"(Acceptance Rate*: underline24%)\n","date":1519516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519516800,"objectID":"aef9689dbe742e31bc9a7f9b5c788a1b","permalink":"https://penn-cil.github.io/publication/khoram-2018-fpga/","publishdate":"2018-02-25T00:00:00Z","relpermalink":"/publication/khoram-2018-fpga/","section":"publication","summary":"Graph analytics, which explores the relationships among interconnected entities, is becoming increasingly important due to its broad applicability, from machine learning to social sciences. However, due to the irregular data access patterns in graph computations, one major challenge for graph processing systems is performance. The algorithms, softwares, and hardwares that have been tailored for mainstream parallel applications are generally not effective for massive, sparse graphs from the real-world problems, due to their complex and irregular structures. To address the performance issues in large-scale graph analytics, we leverage the exceptional random access performance of the emerging Hybrid Memory Cube (HMC) combined with the flexibility and efficiency of modern FPGAs. In particular, we develop a collaborative software/hardware technique to perform a level-synchronized Breadth First Search (BFS) on a FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that exploits the FPGA-HMC platform»s capability to improve data locality and memory access efficiency. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by designing a memory request merging unit to take advantage of the increased data locality resulting from graph clustering. We evaluate the performance of our BFS implementation using the AC-510 development kit from Micron and achieve $2.8 times$ average performance improvement compared to the latest FPGA-HMC based graph processing system over a set of benchmarks from a wide range of applications.","tags":["conference"," graph analytics"," graph clustering"," hardware accelerators"," hybrid memory cube"," reconfigurable logic"," bfs"],"title":"Accelerating  Graph  Analytics  By  Co-Optimizing  Storage  and  Access  on  an FPGA-HMC Platform","type":"publication"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"(Acceptance Rate*: underline24%)\n","date":1519516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519516800,"objectID":"ebaff12575eb8e7b10266528dafcae89","permalink":"https://penn-cil.github.io/publication/zhang-2018-fpga/","publishdate":"2018-02-25T00:00:00Z","relpermalink":"/publication/zhang-2018-fpga/","section":"publication","summary":"Graph traversal is a core primitive for graph analytics and a basis for many higher-level graph analysis methods. However, irregularities in the structure of scale-free graphs (e.g., social network) limit our ability to analyze these important and growing datasets. A key challenge is the redundant graph computations caused by the presence of high-degree vertices which not only increase the total amount of computations but also incur unnecessary random data access. In this paper, we present a graph processing system on an FPGA-HMC platform, based on software/hardware co-design and co- optimization. For the first time, we leverage the inherent graph property i.e. vertex degree to co-optimize algorithm and hardware architecture. In particular, we first develop two algorithm optimization techniques:degree-aware adjacency list reordering anddegree-aware vertex index sorting. The former can reduce the number of redundant graph computations, while the latter can create a strong correlation between vertex index and data access frequency, which can be effectively applied to guide the hardware design. We further implement the optimized hybrid graph traversal algorithm on an FPGA-HMC platform. By leveraging the strong correlation between vertex index and data access frequency made by degree-aware vertex index sorting, we develop two platform-dependent hardware optimization techniques, namely degree-aware data placement and degree-aware adjacency list compression. These two techniques together substantially reduce the amount of access to external memory. Finally, we conduct extensive experiments on an FPGA-HMC platform to verify the effectiveness of the proposed techniques. To the best of our knowledge, our implementation achieves the highest performance (45.8 billion traversed edges per second) among existing FPGA-based graph processing systems.","tags":["conference"," graph processor"," hybrid memory cube"," bfs"],"title":"Degree-aware Hybrid Graph Traversal on FPGA-HMC Platform","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"(Acceptance Rate*: underline24%, Ranked **#1** among 100+ submissions)\n","date":1519516800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519516800,"objectID":"6750073885df7908b702e824f3af47cf","permalink":"https://penn-cil.github.io/publication/zha-2018-fpga/","publishdate":"2018-02-25T00:00:00Z","relpermalink":"/publication/zha-2018-fpga/","section":"publication","summary":"This paper presents a data-centric reconfigurable architecture, namely Liquid Silicon, enabled by emerging non-volatile memory, i.e., RRAM. Compared to the heterogeneous architecture of commercial FPGAs, Liquid Silicon is inherently a homogeneous architecture comprising a two-dimensional (2D) array of identical 'tiles'. Each tile can be configured into one or a combination of four modes: TCAM, logic, interconnect, and memory. Such flexibility allows users to partition resources based on applications? needs, in contrast to the fixed hardware design using dedicated hard IP blocks in FPGAs. In addition to better resource usage, its 'memory friendly' architecture effectively addresses the limitations of commercial FPGAs i.e., scarce on-chip memory resources, making it an effective complement to FPGAs. Moreover, its coarse-grained logic implementation results in shallower logic depth, less inter-tile routing overhead, and thus smaller area and better performance, compared with its FPGA counterpart. Our study shows that, on average, for both traditional and emerging applications, we achieve 62% area reduction, 27% speedup and 31% improvement in energy efficiency when mapping applications onto Liquid Silicon instead of FPGAs.","tags":["conference"," monolithic stacking"," non-volatile memory"," processing-in-memory"," reconfigurable architecture"," tcam","liquid silicon"],"title":"Liquid  Silicon:  A Data-Centric Reconfigurable Architecture enabled by RRAM Technology","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1518652800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518652800,"objectID":"58616ebfa2e31d469607eaa5030813e5","permalink":"https://penn-cil.github.io/publication/zha-2018-jolpe/","publishdate":"2018-02-15T00:00:00Z","relpermalink":"/publication/zha-2018-jolpe/","section":"publication","summary":"","tags":["journal"],"title":"Specialization: A New Path towards Low Power (INVITED)","type":"publication"},{"authors":["Rohit Shukla","Soroosh Khoram","Erik Jorgensen","Jing Li","Mikko Lipasti","Stephen Wright"],"categories":[],"content":"","date":1518566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518566400,"objectID":"a97be37af341c0cafd1a78e6fe664941","permalink":"https://penn-cil.github.io/publication/shukla-2018-frontiers/","publishdate":"2018-02-14T00:00:00Z","relpermalink":"/publication/shukla-2018-frontiers/","section":"publication","summary":"Emerging neural hardware substrates, such as IBM's TrueNorth Neurosynaptic System, can provide an appealing platform for deploying numerical algorithms. For example, a recurrent Hopfield neural network can be used to find the Moore-Penrose generalized inverse of a matrix, thus enabling a broad class of linear optimizations to be solved efficiently, at low energy cost. However, deploying numerical algorithms on hardware platforms that severely limit the range and precision of representation for numeric quantities can be quite challenging. This paper discusses these challenges and proposes a rigorous mathematical framework for reasoning about range and precision on such substrates. The paper derives techniques for normalizing inputs and properly quantizing synaptic weights originating from arbitrary systems of linear equations, so that solvers for those systems can be implemented in a provably correct manner on hardware-constrained neural substrates. The analytical model is empirically validated on the IBM TrueNorth platform, and results show that the guarantees provided by the framework for range and precision hold under experimental conditions. Experiments with optical flow demonstrate the energy benefits of deploying a reduced-precision and energy-efficient generalized matrix inverse engine on the IBM TrueNorth platform, reflecting 10× to 100× improvement over FPGA and ARM core baselines.","tags":["journal"],"title":"Computing Generalized Matrix Inverse on Spiking Neural Substrate","type":"publication"},{"authors":["Soroosh Khoram","Yue Zha","Jing Li"],"categories":[],"content":"","date":1514937600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514937600,"objectID":"2ec4a322746c1370f34d5c495e9f1e4b","permalink":"https://penn-cil.github.io/publication/khoram-2018-cal/","publishdate":"2018-01-03T00:00:00Z","relpermalink":"/publication/khoram-2018-cal/","section":"publication","summary":"Associative Processing (AP) is a promising alternative to the Von Neumann model as it addresses the memory wall problem through its inherent in-memory computations. However, because of the countless design parameter choices, comparisons between implementations of two so radically different models are challenging for simulation-based methods. To tackle these challenges, we develop an alternative analytical approach based on a new concept called architecturally-determined complexity. Using this method, we asymptotically evaluate the runtime/storage/energy bounds of the two models, i.e., AP and Von Neumann. We further apply the method to gain more insights into the performance bottlenecks of traditional AP and develop a new machine model named Two Dimensional AP to address these limitations. Finally, we experimentally validate our analytical method and confirm that the simulation results match our theoretical projections.","tags":["journal"," analytical models","complexity theory","computational modeling","computer architecture","parallel processing","runtime","two dimensional displays","analysis of algorithms and problem complexity","associative processors","associative processing","modeling techniques","models of computation"],"title":"An Alternative Analytical Approach to Associative Processing (Best of CAL)","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline26%, 105 out of 399)\n","date":1510531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510531200,"objectID":"4498d21ded109dfce1fa5e04cb913328","permalink":"https://penn-cil.github.io/publication/zha-2017-iccad/","publishdate":"2017-11-13T00:00:00Z","relpermalink":"/publication/zha-2017-iccad/","section":"publication","summary":"Recent advances in resistive random-access memory (RRAM) evoke great interests in exploring alternative architectures. One interesting work is a RRAM-based reconfigurable architecture that provides superior programmbility and blurs the boundary between computation and storage, but long-distance routing becomes a performance bottleneck. However, long-distance routing in FPGA is efficiently implemented, but its fine-grained routing structure results in a large routing overhead. In this work, we present a RRAM-based reconfigurable architecture that addresses the routing challenges using hybrid routing, i.e., local and global routing by taking the best advantages of both architectures (prior RRAM-based and FPGA). We also provide a complete CAD framework that exhibits high parallelism and good scalability. Experimental results show that our reconfigurable architecture outperforms both architectures. It achieves a 46.88% reduction in delay and improves the energy efficiency by 66.23% compared with the prior RRAM-based architecture with a slightly increased area overhead. While comparing with FPGA, it reduces the delay and the routing overhead by 36.00% and 50.20%, respectively. Additionally, our CAD framework achieves 5.39x speedup, compared with the prior framework.","tags":["conference"," architecture","delays","field programmable gate arrays","logic functions","routing","switches","tiles","cad framework","hybrid routing","in-memory computing","reconfigurable architecture","liquid silicon"],"title":"RRAM-based reconfigurable in-memory computing architecture with hybrid routing","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1499040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499040000,"objectID":"94461cd7518ce6283760c6246faae644","permalink":"https://penn-cil.github.io/publication/zha-2017-calcma/","publishdate":"2017-07-03T00:00:00Z","relpermalink":"/publication/zha-2017-calcma/","section":"publication","summary":"","tags":["journal"," computer architecture","coprocessors","encoding","ip networks","intrusion detection","ports (computers)","accelerator","intrusion detection","network security","reram","tcam"],"title":"CMA: A Reconfigurable Complex Matching Accelerator for Wire-speed Network Intrusion Detection","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"(Acceptance Rate*: underline29%)\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"940f0d8c1ba705445f419ae0acf8e603","permalink":"https://penn-cil.github.io/publication/zha-2017-dacwip/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/zha-2017-dacwip/","section":"publication","summary":"(Acceptance Rate*: underline29%)","tags":["conference"],"title":"RRAM-based  Reconfigurable  In-Memory  Computing  Architecture with Hybrid Routing (poster)","type":"publication"},{"authors":["Soroosh Khoram","Jialiang Zhang","Maxwell Strange","Jing Li"],"categories":[],"content":"Acceptance rate: underline25%, 32 out of 128\n","date":1493510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493510400,"objectID":"51112a34e2285813ae9f915d46d861e3","permalink":"https://penn-cil.github.io/publication/khoram-2017-fccm/","publishdate":"2017-04-30T00:00:00Z","relpermalink":"/publication/khoram-2017-fccm/","section":"publication","summary":"Acceptance rate: underline25%, 32 out of 128","tags":["conference"," field programmable gate arrays","graph theory","information retrieval","learning (artificial intelligence)","social sciences","tree searching","bfs","fpga-hmc based graph processing system","breadth first search","hybrid memory cube","interconnected entities","irregular data access pattern","large-scale graph analytics","machine learning","massive-scale sparse graphs","social science","acceleration","clustering algorithms","field programmable gate arrays","hardware","merging","software","software algorithms","breadth-first search","graph clustering","hybrid memory cube"],"title":"Accelerating Large-Scale Graph Analytics with FPGA and HMC (Poster)","type":"publication"},{"authors":["Soroosh Khoram","Yue Zha","Jialiang Zhang","Jing Li"],"categories":[],"content":"(Acceptance Rate*: underline35%)\n","date":1489881600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489881600,"objectID":"cd18f8f096fb46856b0c0f086cb0929d","permalink":"https://penn-cil.github.io/publication/khoram-2017-ispd/","publishdate":"2017-03-19T00:00:00Z","relpermalink":"/publication/khoram-2017-ispd/","section":"publication","summary":"The confluence of the recent advances in technology and the ever-growing demand for large-scale data analytics created a renewed interest in a decades-old concept, processing-in-memory (PIM). PIM, in general, may cover a very wide spectrum of compute capabilities embedded in close proximity to or even inside the memory array. In this paper, we present an initial taxonomy for dividing PIM into two broad categories: 1) Near-memory processing and 2) In-memory processing. This paper highlights some interesting work in each category and provides insights into the challenges and possible future directions.","tags":["conference"," 3d integration"," in-memory processing"," near-memory processing"," nonvolatile memory"],"title":"Challenges and Opportunities: From Near-memory Computing to In-memory Computing (INVITED)","type":"publication"},{"authors":["Yue Zha","Zhiqiang Wei","Jing Li"],"categories":[],"content":"","date":1489276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1489276800,"objectID":"445218a39b044b3abfea0e1c6ac178e6","permalink":"https://penn-cil.github.io/publication/zha-2017-cstic/","publishdate":"2017-03-12T00:00:00Z","relpermalink":"/publication/zha-2017-cstic/","section":"publication","summary":"","tags":["conference"," integrated circuit modelling","product development","resistive ram","iv characteristics","rram technology","scm","commercialization progress","compact model","drop-in replacement","embedded memory","essential electrical-chemical-thermal properties","nonvon neumann architecture","product development","standalone memory","storage class memory","switching dynamics","computational modeling","computer architecture","hidden markov models","mathematical model","random access memory","resistance","switches"],"title":"Recent progress in RRAM technology: From compact models to applications (INVITED)","type":"publication"},{"authors":["Yue Zha","Jialiang Zhang","Zhiqiang Wei","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline25%, 25 out of 101)\n","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"10e6d8a7044fa2d03ff14ac3e9b9d5de","permalink":"https://penn-cil.github.io/publication/zha-2017-fpgaposter/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/publication/zha-2017-fpgaposter/","section":"publication","summary":"(Acceptance Rate: underline25%, 25 out of 101)","tags":["conference"," coarse-grained configuration"," mixed-signal processing"," non-volatile memory"," reconfigurable architecture"," ternary content addressable memory"],"title":"A Mixed-Signal Data-Centric Reconfigurable Architecture Enabled by RRAM Technology (poster)","type":"publication"},{"authors":["Jialiang Zhang","Soroosh Khoram","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline25%, 25 out of 101)\n","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"1fc5222d7c756dbd89a973f71b6b09e6","permalink":"https://penn-cil.github.io/publication/zhang-2017-fpga-bfs/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/publication/zhang-2017-fpga-bfs/","section":"publication","summary":"Large graph processing has gained great attention in recent years due to its broad applicability from machine learning to social science. Large real-world graphs, however, are inherently difficult to process efficiently, not only due to their large memory footprint, but also that most graph algorithms entail memory access patterns with poor locality and a low compute-to-memory access ratio. In this work, we leverage the exceptional random access performance of emerging Hybrid Memory Cube (HMC) technology that stacks multiple DRAM dies on top of a logic layer, combined with the flexibility and efficiency of FPGA to address these challenges. To our best knowledge, this is the first work that implements a graph processing system on a FPGA-HMC platform based on software/hardware co-design and co-optimization. We first present the modifications of algorithm and a platform-aware graph processing architecture to perform level-synchronized breadth first search (BFS) on FPGA-HMC platform. To gain better insights into the potential bottlenecks of proposed implementation, we develop an analytical performance model to quantitatively evaluate the HMC access latency and corresponding BFS performance. Based on the analysis, we propose a two-level bitmap scheme to further reduce memory access and perform optimization on key design parameters (e.g. memory access granularity). Finally, we evaluate the performance of our BFS implementation using the AC-510 development kit from Micron. We achieved 166 million edges traversed per second (MTEPS) using GRAPH500 benchmark on a random graph with a scale of 25 and an edge factor of 16, which significantly outperforms CPU and other FPGA-based large graph processors.","tags":["conference","graph processor","hybrid memory cube:bfs"],"title":"Boosting the Performance of FPGA-based Graph Processor Using Hybrid Memory Cube: A Case for Breadth First Search","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"f4772bf6cb71cee11cda79135edaa367","permalink":"https://penn-cil.github.io/publication/zha-2017-calimec/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/publication/zha-2017-calimec/","section":"publication","summary":"","tags":["journal"," decoding","energy efficiency","field programmable gate arrays","nonvolatile memory","program processors","non-volatile memory","tcam","energy-efficiency computing","processing-in-memory"],"title":"IMEC: A Fully Morphable In-Memory Computing Fabric Enabled by Resistive Crossbar","type":"publication"},{"authors":["Jialiang Zhang","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline25%, 25 out of 101)\n","date":1487721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487721600,"objectID":"6e3bc7cc02323c474efa71e1fd6c0119","permalink":"https://penn-cil.github.io/publication/zhang-2017-fpga-cnn/","publishdate":"2017-02-22T00:00:00Z","relpermalink":"/publication/zhang-2017-fpga-cnn/","section":"publication","summary":"OpenCL FPGA has recently gained great popularity with emerging needs for workload acceleration such as Convolutional Neural Network (CNN), which is the most popular deep learning architecture in the domain of computer vision. While OpenCL enhances the code portability and programmability of FPGA, it comes at the expense of performance. The key challenge is to optimize the OpenCL kernels to efficiently utilize the flexible hardware resources in FPGA. Simply optimizing the OpenCL kernel code through various compiler options turns out insufficient to achieve desirable performance for both compute-intensive and data-intensive workloads such as convolutional neural networks.  In this paper, we first propose an analytical performance model and apply it to perform an in-depth analysis on the resource requirement of CNN classifier kernels and available resources on modern FPGAs. We identify that the key performance bottleneck is the on-chip memory bandwidth. We propose a new kernel design to effectively address such bandwidth limitation and to provide an optimal balance between computation, on-chip, and off-chip memory access. As a case study, we further apply these techniques to design a CNN accelerator based on the VGG model. Finally, we evaluate the performance of our CNN accelerator using an Altera Arria 10 GX1150 board. We achieve 866 Gop/s floating point performance at 370MHz working frequency and 1.79 Top/s 16-bit fixed-point performance at 385MHz. To the best of our knowledge, our implementation achieves the best power efficiency and performance density compared to existing work.","tags":["conference"," convolutional neural networks"," fpga"," hardware accelerator"," opencl"],"title":"Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network","type":"publication"},{"authors":["Yue Zha","Jing Li"],"categories":[],"content":"(Acceptance Rate: underline24%, 97 out of 408)\n","date":1478476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478476800,"objectID":"1eaddc45e7cc0ac0157b802e201ac3ed","permalink":"https://penn-cil.github.io/publication/zha-2016-iccad/","publishdate":"2016-11-07T00:00:00Z","relpermalink":"/publication/zha-2016-iccad/","section":"publication","summary":"Driven by recent advances in resistive random-access memory (RRAM), there have been growing interests in exploring alternative computing concept, i.e., in-memory processing, to address the classical von Neumann bottlenecks. Despite of their great promise in improving performance and energy efficiency, most existing works are built on the inherent matrix-vector multiplication capability of RRAM crossbar structure, and thus lack the flexibility to adapt to future market/technology induced changes in data-intensive applications. To address these challenges, we propose an in-memory reconfigurable architecture based on RRAM crossbar structure. For the first time, it achieves a full programmability across computation and storage, and thereby provides more flexibilities of partitioning the hardware resources based on applications' needs. We further develop two complete CAD design flows to facilitate development of applications written in hardware description languages (HDLs) for our architecture, based on: 1) adaption from existing tool set developed for FPGA, 2) a custom tool design optimized towards the new architecture. Our experiments show that, both design flows are effective in exploiting flexible resources offered by our architecture and thus achieves better efficiency than state-of-art FPGAs (30% improvement in performance with 66% reduction in area). In addition, compared to adapted design flow, our custom design flow achieves speedup by 3.3×, and further improves mapping quality.","tags":["conference"," rram"," in-memory computing"," reconfigurable","liquid silicon"],"title":"Reconfigurable in-memory computing with resistive memory crossbar","type":"publication"},{"authors":["Xiaoxin Xu","Qing Luo","Tiancheng Gong","Hangbing Lv","Shibing Long","Qi Liu","Steve S. Chung","Jing Li","Ming Liu"],"categories":[],"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"105b6c1fda7486cc54f99c8c49f16b91","permalink":"https://penn-cil.github.io/publication/xu-2016-vlsi/","publishdate":"2016-06-01T00:00:00Z","relpermalink":"/publication/xu-2016-vlsi/","section":"publication","summary":"","tags":["conference"," cmos memory circuits","integrated circuit manufacture","resistive ram","cmos","rram","self-aligned self-selective cell","size 5 nm","vertical resistive switching memory","etching","hafnium compounds","leakage currents","programming","resistance","three-dimensional displays","threshold voltage"],"title":"Fully CMOS compatible 3D vertical RRAM with self-aligned self-selective cell enabling sub-5nm scaling","type":"publication"},{"authors":["Bochen Guan","Jing Li"],"categories":[],"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"bc41a31f64a39fcdc11b98f11b9361be","permalink":"https://penn-cil.github.io/publication/guan-2016-irps/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/publication/guan-2016-irps/","section":"publication","summary":"","tags":["conference"," monte carlo methods","current fluctuations","electromagnetic interference","integrated circuit design","integrated circuit reliability","random noise","resistive ram","telegraphy","monte carlo method","rram circuit reliability","rram compact model","rtn effect","current fluctuation","random telegraph noise","tunneling gap","current measurement","data models","electron traps","fluctuations","integrated circuit modeling","mathematical model","switches","compact model","rram","random telegraph noise"],"title":"A compact model for RRAM including random telegraph noise","type":"publication"},{"authors":["Qing Luo","Xiaoxin Xu","Hongtao Liu","Hangbing Lv","Tiancheng Gong","Shibing Long","Qi Liu","Haitao Sun","Writam Banerjee","Ling Li","Jianfeng Gao","Nianduan Lu","Steve S. Chung","Jing Li","Ming Liu"],"categories":[],"content":"(Acceptance Rate*: underline33%)\n","date":1448928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448928000,"objectID":"a5a1917502d1102a3dd95a109c89dbdb","permalink":"https://penn-cil.github.io/publication/luo-2015-iedm/","publishdate":"2015-12-01T00:00:00Z","relpermalink":"/publication/luo-2015-iedm/","section":"publication","summary":"(Acceptance Rate*: underline33%)","tags":["conference"," hafnium compounds","ionic conductivity","leakage currents","mixed conductivity","resistive ram","3d vertical rram","hfo2","hfo2/mixed ionic and electronic conductor bilayer","four-layer v-rram array","high selectivity","nonlinearity","operation current","self-compliance memory cells","self-selective cell","ultra low-leakage","ultra-low half-select leakage","hafnium compounds","leakage currents","optical switches","resistance","three-dimensional displays","tin"],"title":"Demonstration of 3D vertical RRAM with ultra low-leakage, high-selectivity and self-compliance memory cells","type":"publication"},{"authors":["Jing Li"],"categories":[],"content":"","date":1430438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430438400,"objectID":"1deb01d36e31cae56259eda5b63525f3","permalink":"https://penn-cil.github.io/publication/li-2015-iscas/","publishdate":"2015-05-01T00:00:00Z","relpermalink":"/publication/li-2015-iscas/","section":"publication","summary":"","tags":["conference"," big data","computer centres","content-addressable storage","memory architecture","phase change memories","big data problems","nvm technology","pcm technology","tcam","computing stack","cost-per-bit factor","data manipulation","data storage","data-centric computing","data-intensive applications","endurance factor","hardware features","nonvolatile memory technology","performance factor","phase-change memory","power factor","retention factor","ternary content addressable memory","encoding","hardware","nonvolatile memory","phase change materials","phase change memory","random access memory","reliability","emerging nonvolatile memory","pcm","tcam","ternary content addressable memory","data-centric system","near-/in-memory computing","phase change memory"],"title":"Enabling phase-change memory for data-centric computing: Technology, circuitand system (INVITED)","type":"publication"},{"authors":["Jing Li","Robert Montoye","Masatoshi Ishii","Leland Chang"],"categories":[],"content":"","date":1396310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396310400,"objectID":"62e0cfd71c1a07b5976c5cbf77c50469","permalink":"https://penn-cil.github.io/publication/li-2014-jssc/","publishdate":"2014-04-01T00:00:00Z","relpermalink":"/publication/li-2014-jssc/","section":"publication","summary":"This work demonstrates the first fabricated 1 Mb nonvolatile TCAM using 2-transistor/2-resistive-storage (2T-2R) cells to achieve 10× smaller cell size than SRAM-based TCAMs at the same technology node. The test chip was designed and fabricated in IBM 90 nm CMOS technology and mushroom phase-change memory (PCM) technology. The primary challenge for enabling reliable array operation with such aggressive cell is presented, namely, severely degraded sensing margin due to significantly lower ON/OFF ratio of resistive memories (~10^2 for PCM) than that of traditional MOSFETs (10^5 ). To address this challenge, two enabling techniques were developed and implemented in hardware: 1) two-bit encoding and 2) a clocked self-referenced sensing scheme (CSRSS). In addition, the two-bit encoding can also improve algorithmic mapping by effectively compressing TCAM entries. The 1 Mb chip demonstrates reliable low voltage search operation (VDDmin ~750 mV) and a match delay of 1.9 ns under nominal operating conditions.","tags":["journal"," content-addressable storage","encoding","phase change memories","2t 2r cell nonvolatile tcam","cmos technology","algorithmic mapping","clocked self referenced sensing","phase change memory technology","resistive memories","size 90 nm","time 1.9 ns","two bit encoding","arrays","encoding","microprocessors","phase change materials","random access memory","sensors","associative computing","encoding","hardware accelerator","intrusion detection","matchline compensation","nonvolatile","packet classification","phase change memory (pcm)","search engine","self-referenced sensing","ternary content addressable memory (tcam)"],"title":"1 Mb 0.41 um^2 2T-2R cell nonvolatile TCAM with two-bit encoding and clocked self-referenced sensing (INVITED)","type":"publication"},{"authors":["Jing Li","Robert Montoye","Masatoshi Ishii","Kevin Stawiasz","Takeshi Nishida","Kim Maloney","Gary Ditlow","Scott Lewis","Tom Maffitt","Richard Jordan","others"],"categories":[],"content":"(Acceptance Rate: underline27%, 109 out of 396)\n","date":1370995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370995200,"objectID":"1903e57a1c6dbed8f8eb9eb046dd62be","permalink":"https://penn-cil.github.io/publication/li-2013-vlsi/","publishdate":"2013-06-12T00:00:00Z","relpermalink":"/publication/li-2013-vlsi/","section":"publication","summary":"This work demonstrates the first fabricated nonvolatile TCAM using 2-transistor/2-resistive-storage (2T-2R) cells to achieve 10× smaller cell size than SRAM-based TCAMs at the same technology node. The test chip was designed and fabricated in IBM 90nm CMOS technology and mushroom phase-change memory (PCM) process. To ensure reliable search operation with such compact cells, two enabling techniques were developed and implemented in hardware: 1) two-bit encoding, and 2) a clocked self-referenced sensing scheme (CSRSS). The 1Mb chip demonstrates reliable low voltage search operation (VDDmin~750mV) and a match delay of 1.9 ns under nominal operating conditions.","tags":["conference"," cmos memory circuits","sram chips","clocks","content-addressable storage","integrated circuit design","integrated circuit reliability","low-power electronics","phase change memories","search problems","2-transistor-2-resistive-storage cells","2t-2r cells","csrss","ibm cmos technology","pcm process","sram-based tcam","bit rate 1 mbit/s","cell nonvolatile tcam","cell size","clocked self-referenced sensing scheme","compact cells","fabricated nonvolatile tcam","low voltage search operation","match delay","mushroom phase-change memory process","reliable search operation","size 90 nm","technology node","test chip design","two-bit encoding","arrays","clocks","encoding","microprocessors","phase change materials","sensors"],"title":"1Mb 0.41 um^2 2T-2R cell nonvolatile TCAM with two-bit encoding and clocked self-referenced sensing (Highlight Paper of the Year)","type":"publication"},{"authors":["K Cil","Y Zhu","Jing Li","CH Lam","H Silva"],"categories":[],"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1356998400,"objectID":"e3647c52068044d6b84061efe61908d3","permalink":"https://penn-cil.github.io/publication/cil-2013-thinfilm/","publishdate":"2013-01-01T00:00:00Z","relpermalink":"/publication/cil-2013-thinfilm/","section":"publication","summary":"","tags":["phase change memory"," germanium–antimony–tellurium"," phase transition temperature"," face-centered cubic"," hexagonal close-packed"," substrate dependence"," silicon nitride"," silicon dioxide"],"title":"Assisted cubic to hexagonal phase transition in GeSbTe thin films on silicon nitride","type":"publication"},{"authors":["Justin Meza","Jing Li","Onur Mutlu"],"categories":[],"content":"SAFARI Technical Report\n","date":1354320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1354320000,"objectID":"7b3405708f6aff591b8d61b89c7f7fef","permalink":"https://penn-cil.github.io/publication/meza-2012-report-nvm/","publishdate":"2012-12-01T00:00:00Z","relpermalink":"/publication/meza-2012-report-nvm/","section":"publication","summary":"SAFARI Technical Report","tags":["techreport"],"title":"Evaluating Row Buffer Locality in Future Non-Volatile Main Memories","type":"publication"},{"authors":["Xiao Zhang","Jerome Mitard","Lars-Ake Ragnarsson","Tomas Hoffmann","Michael Deal","Melody E. Grubbs","Jing Li","Blanka Magyari-Kope","Bruce M. Clemens","Yoshio Nishi"],"categories":[],"content":"","date":1351728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1351728000,"objectID":"33d02635b5a88f98d3fbf31f3ba87853","permalink":"https://penn-cil.github.io/publication/zhang-2012-ted/","publishdate":"2012-11-01T00:00:00Z","relpermalink":"/publication/zhang-2012-ted/","section":"publication","summary":"","tags":["journal"," mosfet","failure analysis","probability","random-access storage","semiconductor device models","semiconductor device reliability","mos devices","mosfet","wfv","grain orientation","polycrystalline metal gate","random dopant fluctuation","size 22 nm","static ram failure probability","threshold voltage variability","work-function variability","integrated circuit modeling","logic gates","random access memory","resource description framework","semiconductor device modeling","mosfets","metal gate","variability","work function (wf)"],"title":"Theory and Experiments of the Impact of Work-Function Variability on Threshold Voltage Variability in MOS Devices","type":"publication"},{"authors":["Justin Meza","Jing Li","Onur Mutlu"],"categories":[],"content":"(Acceptance rate: underline25%, 61 out of 241)\n","date":1348963200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1348963200,"objectID":"df7703afce33aa264de51ad01a2cc41c","permalink":"https://penn-cil.github.io/publication/meza-2012-iccd/","publishdate":"2012-09-30T00:00:00Z","relpermalink":"/publication/meza-2012-iccd/","section":"publication","summary":"DRAM-based main memories have read operations that destroy the read data, and as a result, must buffer large amounts of data on each array access to keep chip costs low. Unfortunately, system-level trends such as increased memory contention in multi-core architectures and data mapping schemes that improve memory parallelism lead to only a small amount of the buffered data to be accessed. This makes buffering large amounts of data on every memory array access energy-inefficient; yet organizing DRAM chips to buffer small amounts of data is costly, as others have shown. Emerging non-volatile memories (NVMs) such as PCM, STT-RAM, and RRAM, however, do not have destructive read operations, opening up opportunities for employing small row buffers without incurring additional area penalty and/or design complexity. In this work, we discuss and evaluate architectural changes to enable small row buffers at a low cost in NVMs. We find that on a multi-core system, reducing the row buffer size can greatly reduce main memory dynamic energy compared to a DRAM baseline with large row sizes, without greatly affecting endurance, and for some NVM technologies, leads to improved performance.","tags":["conference"," dram chips","buffer circuits","multiprocessing systems","dram baseline","dram chips","dram-based main memories","nvm technologies","array access","buffered data","chip costs","data mapping schemes","main memory dynamic energy","memory array access","memory parallelism","multicore architectures","nonvolatile main memories","read operations","row buffer size","small row buffers","system-level trends","arrays","memory management","nonvolatile memory","organizations","phase change materials","random access memory"],"title":"A case for small row buffers in non-volatile main memories","type":"publication"},{"authors":["Adam Cywar","Jing Li","Chung Lam","Helena Silva"],"categories":[],"content":"","date":1336608000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1336608000,"objectID":"70fec089a6adde6bd1e1fe7fbc1451c1","permalink":"https://penn-cil.github.io/publication/cywar-2012-nano/","publishdate":"2012-05-10T00:00:00Z","relpermalink":"/publication/cywar-2012-nano/","section":"publication","summary":"","tags":["journal"],"title":"The impact of heater-recess and load matching in phase change memory mushroom cells","type":"publication"},{"authors":["S. Kim","P. Y. Du","Jing Li","M. Breitwisch","Y. Zhu","S. Mittal","R. Cheek","T. H. Hsu","M. H. Lee","A. Schrott","S. Raoux","H. Y. Cheng","S. C. Lai","J. Y. Wu","T. Y. Wang","E. A. Joseph","E. K. Lai","A. Ray","H. L. Lung","C. Lam"],"categories":[],"content":"","date":1333238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1333238400,"objectID":"e54d0bb4ebc795cbbe5fa697322c7be1","permalink":"https://penn-cil.github.io/publication/kim-2012-vlsitsa/","publishdate":"2012-04-01T00:00:00Z","relpermalink":"/publication/kim-2012-vlsitsa/","section":"publication","summary":"","tags":["conference"," failure analysis","phase change memories","reset current margin","endurance cycles","endurance failure modes","material segregation effect","open failure","optimization","phase change memory","phase-dependent open-failure mechanisms","programming conditions","programming current","stuck-set failure characteristic curves","current density","optimization","phase change materials","phase change memory","programming","resistance"],"title":"Optimization of programming current on endurance of phase change memory","type":"publication"},{"authors":["Jing Li","Binquan Luan","Chung Lam"],"categories":[],"content":"","date":1333238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1333238400,"objectID":"b2a817aec88d11bd51aec34ab4378cde","permalink":"https://penn-cil.github.io/publication/li-2012-irps/","publishdate":"2012-04-01T00:00:00Z","relpermalink":"/publication/li-2012-irps/","section":"publication","summary":"","tags":["conference"," circuit reliability","molecular dynamics method","phase change memories","mlc pcm","sr","amorphous chalcogenide material","atomic structure","material engineering","mitigation technique","phase change memory","physics model","quantum molecular dynamic simulation","reliability issue","structural relaxation","time dependent resistance drift","annealing","kinetic theory","phase change materials","resistance","strontium","temperature measurement","drift","multi-level cell","phase change memory","structural relaxation"],"title":"Resistance drift in phase change memory (INVITED)","type":"publication"},{"authors":["Pei-Ying Du","J. Y. Wu","T. H. Hsu","M. H. Lee","T. Y. Wang","H. Y. Cheng","E. K. Lai","S. C. Lai","H. L. Lung","S. Kim","M. J. BrightSky","Y. Zhu","S. Mittal","R. Cheek","S. Raoux","E. A. Joseph","A. Schrott","Jing Li","Chung Lam"],"categories":[],"content":"","date":1333238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1333238400,"objectID":"6ec55550b8f5d6e169c67bfa50a37120","permalink":"https://penn-cil.github.io/publication/du-2012-irps/","publishdate":"2012-04-01T00:00:00Z","relpermalink":"/publication/du-2012-irps/","section":"publication","summary":"","tags":["conference"," arrays","circuit reliability","electromigration","melting","phase change memories","segregation","gst-based phase change memory","reset melting healing effect","set induced damage","set operation","control circuits","electromigration","large test chips","operation impact","phase change memory reliability","phase segregation","reset operation","conductivity","electromigration","maintenance engineering","phase change materials","phase change memory","resistance","tin","endurance","reset operation","electromigration","melting","phasechange memory (pcm)","reliability","segregation"],"title":"The impact of melting during reset operation on the reliability of phase change memory","type":"publication"},{"authors":["J. Y. Wu","M. Breitwisch","S. Kim","T. H. Hsu","R. Cheek","P. Y. Du","Jing Li","E. K. Lai","Y. Zhu","T. Y. Wang","H. Y. Cheng","A. Schrott","E. A. Joseph","R. Dasaka","S. Raoux","M. H. Lee","H. L. Lung","C. Lam"],"categories":[],"content":"(Acceptance Rate*: underline33%)\n","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"997d16992b53d04740f1e159da96056f","permalink":"https://penn-cil.github.io/publication/wu-2011-iedm/","publishdate":"2011-12-01T00:00:00Z","relpermalink":"/publication/wu-2011-iedm/","section":"publication","summary":"(Acceptance Rate*: underline33%)","tags":["conference"," conductors (electric)","electrodes","heat losses","integrated circuit reliability","low-power electronics","phase change memories","tantalum compounds","thermal insulation","titanium compounds","tan-tin","current 30 mua","electrical conductivity","electrothermal simulation","low power pcm","low power phase change memory","size 1.5 nm","size 39 nm","storage capacity 256 mbit","thermal barrier","thermal insulation","thermally confined bottom electrode","electrodes","heating","phase change memory","solids","thermal resistance","tin"],"title":"A low power phase change memory using thermally confined TaN/TiN bottom electrode","type":"publication"},{"authors":["Jing Li","Binquan Luan","T. H. Hsu","Y. Zhu","G. Martyna","D. Newns","H. Y. Cheng","S. Raoux","H. L. Lung","C. Lam"],"categories":[],"content":"(Acceptance Rate*: underline33%)\n","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1322697600,"objectID":"e00e0971f56e4f647c5613228965edcf","permalink":"https://penn-cil.github.io/publication/li-2011-iedm/","publishdate":"2011-12-01T00:00:00Z","relpermalink":"/publication/li-2011-iedm/","section":"publication","summary":"(Acceptance Rate*: underline33%)","tags":["conference"," amorphous semiconductors","antimony alloys","atomic structure","germanium alloys","phase change materials","phase change memories","tellurium alloys","ge","sb","te","amorphous germanium","atomic structure","drift-insensitive phase change material","electrical characteristics","first principle ab initio method","material-device characterization","phase change memory","resistance drift","tellurium ternary alloys","conductivity","phase change materials","phase change memory","programming","resistance","temperature measurement"],"title":"Explore physical origins of resistance drift in phase change memory and its implication for drift-insensitive materials","type":"publication"},{"authors":["Simone Raoux","Huai-Yu Cheng","Jury Sandrini","Jing Li","Jean Jordan-Sweet"],"categories":[],"content":"","date":1320105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1320105600,"objectID":"5c2b94ace6aa25e1708d943c4ab1fd2f","permalink":"https://penn-cil.github.io/publication/raoux-2011-nvmts/","publishdate":"2011-11-01T00:00:00Z","relpermalink":"/publication/raoux-2011-nvmts/","section":"publication","summary":"","tags":["conference"," x-ray diffraction","antimony alloys","crystallisation","germanium alloys","phase change materials","phase change memories","tellurium alloys","gesbte","amorphous phase","crystallization temperature","electrical contrast","materials ewngineering","phase change random access memory","rhombohedral phase","temperature 200 degc","time resolved x-ray diffraction","phase change materials","phase change random access memory"],"title":"Materials engineering for Phase Change Random Access Memory","type":"publication"},{"authors":[" Cheng-Yuan Wen","Jeyanandh Paramesh","Larry Pileggi","Jing Li","SangBum Kim","Jonathan Proesel","Chung Lam"],"categories":[],"content":"(Acceptance Rate: underline38%, 121 out of 314)\n","date":1314835200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1314835200,"objectID":"c87e2acafe0f5dbc164ff2422a755176","permalink":"https://penn-cil.github.io/publication/wen-2011-esscirc/","publishdate":"2011-09-01T00:00:00Z","relpermalink":"/publication/wen-2011-esscirc/","section":"publication","summary":"(Acceptance Rate: underline38%, 121 out of 314)","tags":["conference"," cmos analogue integrated circuits","antimony compounds","calibration","chalcogenide glasses","comparators (circuits)","elemental semiconductors","germanium compounds","phase change memories","redundancy","silicon","tellurium compounds","ge2sb2te5","ibm cmos technology","pcram mushroom cells","si","analog cmos","capacitance 4.41 ff","combinatorial redundancy","digital calibration","embedded gst","nonvolatile phase-change random access memory cells","offset-minimized cmos comparator","post-manufacturing calibration","post-silicon calibration","power 55.42 muw","size 90 nm","switchable resistances","voltage 1 v","arrays","cmos integrated circuits","calibration","generators","phase change random access memory","redundancy","resistance"],"title":"Post-silicon calibration of analog CMOS using phase-change memory cells","type":"publication"},{"authors":["C. Y. Wen","Jing Li","S. Kim","M. Breitwisch","C. Lam","J. Paramesh","L. T. Pileggi"],"categories":[],"content":"(Acceptance Rate: underline28%, 115 out of 409)\n","date":1308096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1308096000,"objectID":"2bae4a9f32c92906d560e4c738f60421","permalink":"https://penn-cil.github.io/publication/wen-2011-vlsi/","publishdate":"2011-06-15T00:00:00Z","relpermalink":"/publication/wen-2011-vlsi/","section":"publication","summary":"(Acceptance Rate: underline28%, 115 out of 409)","tags":["conference"," cmos memory circuits","antimony compounds","chalcogenide glasses","germanium compounds","logic circuits","phase change memories","programmable circuits","random-access storage","tellurium compounds","cmos technology","ge2sb2te5","pcm mushroom cell","digital look-up table circuit","nonvolatile logic functions","nonvolatile look-up table design","phase-change memory","programmable logic functions","resistance transformation ratio","size 90 nm","voltage 1 v","cmos integrated circuits","logic gates","phase change materials","phase change random access memory","resistance","table lookup"],"title":"A non-volatile look-up table design using PCM (phase-change memory) cells","type":"publication"},{"authors":["Jing Li","C. I. Wu","S. C. Lewis","J. Morrish","T. Y. Wang","R. Jordan","T. Maffitt","M. Breitwisch","A. Schrott","R. Cheek","H. L. Lung","C. Lam"],"categories":[],"content":"","date":1304208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304208000,"objectID":"3b2aa874dad171e7f276c68a0a2953c0","permalink":"https://penn-cil.github.io/publication/li-2011-imw/","publishdate":"2011-05-01T00:00:00Z","relpermalink":"/publication/li-2011-imw/","section":"publication","summary":"","tags":["conference"," cmos digital integrated circuits","nand circuits","flash memories","phase change memories","2mcell pcm chip","cmos technology","nand flash","analog resistance levels","frequency 50 mhz","phase change memory","reconfigurable sensing scheme","size 90 nm","time 35 mus to 50 mus","time 5 mus","variable level storage","word length 8 bit","clocks","electrical resistance measurement","flash memory","phase change materials","radiation detectors","resistance"],"title":"A Novel Reconfigurable Sensing Scheme for Variable Level Storage in Phase Change Memory","type":"publication"},{"authors":["B. Rajendran","R. W. Cheek","L. A. Lastras","M. M. Franceschini","M. J. Breitwisch","A. G. Schrott","Jing Li","R. K. Montoye","L. Chang","C. Lam"],"categories":[],"content":"","date":1304208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304208000,"objectID":"3e965a55d34b29fabc00a66e1e85ee85","permalink":"https://penn-cil.github.io/publication/rajendran-2011-imw/","publishdate":"2011-05-01T00:00:00Z","relpermalink":"/publication/rajendran-2011-imw/","section":"publication","summary":"","tags":["conference"," monte carlo methods","content-addressable storage","phase change memories","monte-carlo simulation","pcm decives","sram","tcam","content addressable memory","phase change devices","phase change memory technology","ternary cam","arrays","computer aided manufacturing","fets","phase change materials","programming","resistance","resistors"],"title":"Demonstration of CAM and TCAM Using Phase Change Devices","type":"publication"},{"authors":["Jing Li","Chung Lam"],"categories":[],"content":"","date":1304208000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1304208000,"objectID":"57949e7f208790875189c6ff699e313d","permalink":"https://penn-cil.github.io/publication/li-2011-sciencechina/","publishdate":"2011-05-01T00:00:00Z","relpermalink":"/publication/li-2011-sciencechina/","section":"publication","summary":"Phase change memory (PCM) is a non-volatile solid-state memory technology based on the large resistivity contrast between the amorphous and crystalline states in phase change materials. We present the physics behind this large resistivity contrast and describe how it is being exploited to create high density PCM. We address the challenges facing this technology, including the design of PCM cells, fabrication, device variability, thermal cross-talk and write disturb. We discuss the scalability, assess the performance, and examine the reliability of PCM including data retention, multi-bit storage and endurance.","tags":["journal"],"title":"Phase change memory (INVITED)","type":"publication"},{"authors":["Jing Li","Patrick Ndai","Ashish Goel","Sayeef Salahuddin","Kaushik Roy"],"categories":[],"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1291161600,"objectID":"7adaeca5424459b7ba02455dad34f084","permalink":"https://penn-cil.github.io/publication/li-2010-tvlsi/","publishdate":"2010-12-01T00:00:00Z","relpermalink":"/publication/li-2010-tvlsi/","section":"publication","summary":"","tags":["journal"," integrated circuit design","magnetic storage","random-access storage","high memory yield","parametric failures","process variations","robust spin-torque transfer magnetic ram","circuit stability","costs","failure analysis","flash memory","magnetic circuits","performance analysis","random access memory","read-write memory","robustness","scalability","spin-torque transfer (stt)","magnetic ram (mram)","memory yield","parametric failures"],"title":"Design Paradigm for Robust Spin-Torque Transfer Magnetic RAM (STT MRAM) From Circuit/Architecture Perspective (Best Paper)","type":"publication"},{"authors":["Yiran Chen","Hai Li","Cheng-Kok Koh","Guangyu Sun","Jing Li","Yuan Xie","Kaushik Roy"],"categories":[],"content":"","date":1288569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1288569600,"objectID":"968e1cb994e7f7885925d32ac43ee117","permalink":"https://penn-cil.github.io/publication/chen-2010-tvlsi/","publishdate":"2010-11-01T00:00:00Z","relpermalink":"/publication/chen-2010-tvlsi/","section":"publication","summary":"","tags":["journal"," adders","digital arithmetic","integrated circuit design","logic design","ic design","nbti tolerance","circuit delay","digital arithmetic","logic design","negative bias temperature instability","variable-latency adder designs","word length 64 bit","adders","circuits","clocks","delay","negative bias temperature instability","niobium compounds","sun","throughput","titanium compounds","very large scale integration","digital arithmetic","ic design","logic design"],"title":"Variable-Latency Adder (VL-Adder) Designs for Low Power and NBTI Tolerance","type":"publication"},{"authors":["Xiao Zhang","Jing Li","M. Grubbs","M. Deal","B. Magyari-Köpe","B. M. Clemens","Y. Nishi"],"categories":[],"content":"(Acceptance Rate*: underline33%)\n","date":1259625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1259625600,"objectID":"9b3768bc1f82d2777be2d419839c3add","permalink":"https://penn-cil.github.io/publication/zhang-2009-iedm/","publishdate":"2009-12-01T00:00:00Z","relpermalink":"/publication/zhang-2009-iedm/","section":"publication","summary":"(Acceptance Rate*: underline33%)","tags":["conference"," mos integrated circuits","mosfet","sram chips","integrated circuit metallisation","integrated circuit reliability","work function","sram reliability","dual metal gate mosfet","grain orientation difference","metal grain work function variability","polycrystalline metal gate","size 22 nm","charge carrier density","circuit analysis","electrodes","fluctuations","high k dielectric materials","mosfets","predictive models","random access memory","resource description framework","semiconductor process modeling"],"title":"Physical model of the impact of metal grain work function variability on emerging dual metal gate MOSFETs and its implication for SRAM reliability","type":"publication"},{"authors":["Jing Li"," Patrick  Ndai","Goel  Ashish"," Kaushik  Roy"],"categories":[],"content":"","date":1235865600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1235865600,"objectID":"d63d1a07625fa569822dc8f24b7b6094","permalink":"https://penn-cil.github.io/publication/li-2009-gsrc/","publishdate":"2009-03-01T00:00:00Z","relpermalink":"/publication/li-2009-gsrc/","section":"publication","summary":"","tags":["conference"],"title":"Variation  Resilient  Spin  Torque  Transfer  MRAM (poster)","type":"publication"},{"authors":["Jing Li","Patrick Ndai","Ashish Goel","Haixin Liu","Kaushik Roy"],"categories":[],"content":"(Acceptance Rate: underline33%, 116 out of 355)\n","date":1232323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1232323200,"objectID":"9b790a12403eeebb4660a90a779d3ed7","permalink":"https://penn-cil.github.io/publication/li-2009-aspdac/","publishdate":"2009-01-19T00:00:00Z","relpermalink":"/publication/li-2009-aspdac/","section":"publication","summary":"Spin-Torque Transfer Magnetic RAM (STT MRAM) is a promising candidate for future embedded applications. It provides desirable memory attributes such as fast access time, low cost, high density and non-volatility. However, variations in process parameters can lead to a large number of cells to fail, severely affecting the yield of the memory array. In this paper, we provide a thorough analysis of the impact of design parameters on parametric failures due to process variations. To achieve high memory yield without incurring expensive technology modification, we developed an alternate design paradigm ---circuit/architecture co-design --- to take advantage of different levels of design hierarchy (circuit and architecture) to improve the yield and memory density. The technique decouples the conflicting design requirements for read stability/writability and density. Consequently, the memory cell failure probability reduces by 48% and cell area reduces by 21% with negligible performance degradation (~0.4%).","tags":["conference"," stt mram"],"title":"An Alternate Design Paradigm for Robust Spin-torque Transfer Magnetic RAM (STT MRAM) from Circuit/Architecture Perspective","type":"publication"},{"authors":["Jing Li"],"categories":[],"content":"Advisor: Prof. Kaushik Roy\n","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"7d0b0b3feb397831f6369fe74ac5f295","permalink":"https://penn-cil.github.io/publication/li-2009-phd/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/li-2009-phd/","section":"publication","summary":"Advisor: Prof. Kaushik Roy","tags":["phd"],"title":"Robust and Energy-efficient Heterogeneous System Design in Emerging Technologies (**Best Thesis Award nominee**)","type":"publication"},{"authors":["Jing Li","Kaushik Roy"],"categories":[],"content":"(Acceptance Rate: underline22%, 148 out of 684)\n","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"e353cdfc56fb2410cbc4bf741614b6d4","permalink":"https://penn-cil.github.io/publication/li-2009-dac/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/li-2009-dac/","section":"publication","summary":"(Acceptance Rate: underline22%, 148 out of 684)","tags":["conference"],"title":"Robust Heterogeneous System Design in Spintronics: Error Resilient Spin Torque MRAM (STT MRAM) Design","type":"publication"},{"authors":["Jing Li","Kunhyuk Kang","Kaushik Roy"],"categories":[],"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"4bb827c017ff251b030f0b874aeacb73","permalink":"https://penn-cil.github.io/publication/li-2009-tcad/","publishdate":"2009-01-01T00:00:00Z","relpermalink":"/publication/li-2009-tcad/","section":"publication","summary":"","tags":["journal"," cmos integrated circuits","circuit reliability","elemental semiconductors","low-power electronics","silicon","statistical analysis","thin film transistors","cmos technology","si","circuit reliability","compensation technique","delay variation","four-finger structure","inverter chain","low-power low-cost application","low-temperature polycrystalline-silicon thin-film transistor","multifinger design technique","multimodal delay distribution","response surface method","statistical simulation methodology","unimodal distribution","variation estimation","cmos logic circuits","cmos technology","circuit simulation","delay","grain boundaries","logic devices","response surface methodology","robustness","substrates","thin film transistors","grain boundary (gb)","low-temperature polycrystalline-silicon (ltps)","process variation","thin-film transistor (tft)"],"title":"Variation Estimation and Compensation Technique in Scaled LTPS TFT Circuits for Low-Power Low-Cost Applications","type":"publication"},{"authors":["Jing Li","Haixin Liu","S. Salahuddin","Kaushik Roy"],"categories":[],"content":"","date":1221955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1221955200,"objectID":"7c0e4033340f4809e8dcf2f8c998b3ff","permalink":"https://penn-cil.github.io/publication/li-2008-cicc/","publishdate":"2008-09-21T00:00:00Z","relpermalink":"/publication/li-2008-cicc/","section":"publication","summary":"","tags":["conference"," green's function methods","mram devices","dram","sram","flash memories","nonequilibrium green's function","optimization","variation-tolerant spin-torque transfer mram array","yield enhancement","circuit simulation","circuit stability","circuit synthesis","electrodes","green's function methods","magnetic tunneling","random access memory","read-write memory","robust stability","scalability"],"title":"Variation-tolerant Spin-Torque Transfer (STT) MRAM array for yield enhancement","type":"publication"},{"authors":["Jing Li","Aditya Bansal","Swarop Ghosh","Kaushik Roy"],"categories":[],"content":"","date":1217548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1217548800,"objectID":"4bfd5b3a0cf6169ec4a9e94ed98b5b0b","permalink":"https://penn-cil.github.io/publication/li-2008-jetc/","publishdate":"2008-08-01T00:00:00Z","relpermalink":"/publication/li-2008-jetc/","section":"publication","summary":"","tags":["3d integration"," bist"," dft"," low-temperature polycrystalline silicon (ltps)"," generic"," grain boundary (gb)"," hybrid system"," inherent variation"," reconfigurable"," thin-film transistor (tft)"],"title":"An Alternate Design Paradigm for Low-power, Low-cost, Testable Hybrid Systems Using Scaled LTPS TFTs (INVITED)","type":"publication"},{"authors":["Jing Li","Charles Augustine","Sayeef Salahuddin","Kaushik Roy"],"categories":[],"content":"(Acceptance Rate: underline23%, 147 out of 639)\n","date":1212883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1212883200,"objectID":"f3b1c6ebc55f7f3c805bf996c4d26d67","permalink":"https://penn-cil.github.io/publication/li-2008-dac/","publishdate":"2008-06-08T00:00:00Z","relpermalink":"/publication/li-2008-dac/","section":"publication","summary":"(Acceptance Rate: underline23%, 147 out of 639)","tags":["conference"," failure analysis","magnetic storage","magnetoelectronics","optimisation","random-access storage","coupled electromagnetic dynamics","failure probability","on-chip embedded memories","spin-torque transfer magnetic random access memory","spintronic device","statistical optimization methodology","yield enhancement","couplings","failure analysis","flash memory","magnetic analysis","magnetic devices","predictive models","probability","random access memory","read-write memory","scalability","stt mram","yield"],"title":"Modeling of failure probability and statistical design of Spin-Torque Transfer Magnetic Random Access Memory (STT MRAM) array for yield enhancement","type":"publication"},{"authors":["Jing Li"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"780eff9fac6455e5552157d10d02fe2c","permalink":"https://penn-cil.github.io/publication/li-2008-report-edram/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/li-2008-report-edram/","section":"publication","summary":"","tags":["techreport"],"title":"Body History Study on 12S eDRAM Sensing Operation","type":"publication"},{"authors":["Jing Li"," Kaushik  Roy"],"categories":[],"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1199145600,"objectID":"611698cded8c0e2aeab505f50df2fd11","permalink":"https://penn-cil.github.io/publication/li-2008-techcon/","publishdate":"2008-01-01T00:00:00Z","relpermalink":"/publication/li-2008-techcon/","section":"publication","summary":"","tags":["conference"],"title":"Modeling of Failure Probability and Statistical Design of Spin-Torque Transfer Magnetic RAM (STT MRAM) Array for Yield Enhancement","type":"publication"},{"authors":["Jing Li","Aditya Bansal","Kaushik Roy"],"categories":[],"content":"","date":1193875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1193875200,"objectID":"e05e09f7a79752f814ab791c7994c867","permalink":"https://penn-cil.github.io/publication/li-2007-ted/","publishdate":"2007-11-01T00:00:00Z","relpermalink":"/publication/li-2007-ted/","section":"publication","summary":"","tags":["journal"," elemental semiconductors","low-power electronics","silicon","silicon-on-insulator","thin film transistors","ltps tft","soi","si - interface","driving current","low-temperature polycrystalline-silicon thin-film transistors","midgap trap density","poly-si thin-film transistors","silicon-on-insulator","single-crystalline silicon","submicrometer ultralow-power digital operation","ultralow-power subthreshold operation","costs","design methodology","design optimization","energy consumption","fabrication","glass","polymers","silicon","substrates","thin film transistors","grain boundary (gb)","low-pressure chemical vapor deposition (lpcvd)","low-temperature polycrystalline silicon (ltps)","thin-film transistor (tft)"],"title":"Poly-Si Thin-Film Transistors: An Efficient and Low-Cost Option for Digital Operation","type":"publication"},{"authors":["Jing Li","S. Ghosh","Kaushik Roy"],"categories":[],"content":"","date":1191196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1191196800,"objectID":"1e6f1ecf5c1e140ad5ebdf1a26b88977","permalink":"https://penn-cil.github.io/publication/li-2007-itc/","publishdate":"2007-10-01T00:00:00Z","relpermalink":"/publication/li-2007-itc/","section":"publication","summary":"","tags":["conference"," vlsi","built-in self test","design for testability","elemental semiconductors","integrated circuit testing","silicon","thin film transistors","3-d technology","bist components","si","vlsi systems","configurable design-for-test units","generic test structure","low-cost low-temperature integrated poly-silicon tft","process tolerant test structure","reconfigurable test structure","thin film transistors","circuit testing","costs","crystallization","design for testability","silicon","substrates","system testing","temperature","thin film transistors","very large scale integration"],"title":"A generic and reconfigurable test paradigm using Low-cost integrated Poly-Si TFTs","type":"publication"},{"authors":["Yiran Chen","Hai Li","Jing Li","Cheng-Kok Koh"],"categories":[],"content":"(Acceptance Rate: underline39%, 74 out of 192)\n","date":1185926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1185926400,"objectID":"fe9fbfb4c58e2ef90084be641ff0223e","permalink":"https://penn-cil.github.io/publication/li-2007-islped/","publishdate":"2007-08-01T00:00:00Z","relpermalink":"/publication/li-2007-islped/","section":"publication","summary":"(Acceptance Rate: underline39%, 74 out of 192)","tags":["conference"," mosfet","adders","logic design","low-power electronics","nbti-induced delay degradation","nbti-tolerant techniques","vl-adder","arithmetic circuit design","clock edge","energy efficiency","lower-power adder designs","manufacturing costs","nanoscale pmos transistors","negative bias temperature instability","variable-latency adder technique","adders","arithmetic","circuit synthesis","clocks","degradation","delay","mosfets","negative bias temperature instability","niobium compounds","titanium compounds","negative bias temperature instability (nbti)","variable-latency adder (vl-adder)"],"title":"Variable-latency adder (VL-adder): new arithmetic circuit design practice to overcome NBTI","type":"publication"},{"authors":["Jing Li","Kunhyuk Kang","Aditya Bansal","Kaushik Roy"],"categories":[],"content":"(Acceptance Rate*: underline13%)\n","date":1180656000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1180656000,"objectID":"a699f5aba10d1356c02e668795a6d8c2","permalink":"https://penn-cil.github.io/publication/li-2007-dac/","publishdate":"2007-06-01T00:00:00Z","relpermalink":"/publication/li-2007-dac/","section":"publication","summary":"(Acceptance Rate*: underline13%)","tags":["conference"," flexible electronics","low-power electronics","semiconductor device models","silicon","substrates","thin film transistors","gb-tolerant design","flexible substrate","grain boundaries","low power electronics","polycrystalline silicon thin film transistor","ultra low power digital application","design methodology","design optimization","displays","electron traps","grain boundaries","low power electronics","silicon","substrates","temperature","thin film transistors","design","experimentation","grain boundary (gb)","thin film transistor (tft)"],"title":"High Performance and Low Power Electronics on Flexible Substrate","type":"publication"},{"authors":["Jing Li","Kunhyuk Kang","Kaushik Roy"],"categories":[],"content":"","date":1177977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1177977600,"objectID":"f9f800edfc64ce1784593d8ad69a64d2","permalink":"https://penn-cil.github.io/publication/li-2007-icicdt/","publishdate":"2007-05-01T00:00:00Z","relpermalink":"/publication/li-2007-icicdt/","section":"publication","summary":"","tags":["conference"," digital integrated circuits","elemental semiconductors","flexible electronics","grain boundaries","integrated circuit design","low-power electronics","response surface methodology","silicon","thin film transistors","si","battery-operated portable electronics","defect grain boundary region","device-to-device variation","flexible substrate","low-cost digital design","low-temperature polycrystalline silicon thin film transistors","multifinger parallel structure","power dissipation","response surface method","scaled ltps tft","size 200 nm","statistical variation","variation-aware circuit design","voltage 10 v to 20 v","circuit synthesis","digital circuits","flexible printed circuits","glass","grain boundaries","polymers","silicon","substrates","temperature","thin film transistors","low-temperature polycrystalline-silicon (ltps)","response surface method (rsm)","grain boundary (gb)","thin film transistor (tft)"],"title":"Novel Variation-Aware Circuit Design of Scaled LTPS TFT for Ultra low Power, Low-Cost Applications","type":"publication"},{"authors":["Jing Li"," Kaushik  Roy"],"categories":[],"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"3c688c4b7b01eb58b7c4992804695b84","permalink":"https://penn-cil.github.io/publication/li-2007-techcon/","publishdate":"2007-01-01T00:00:00Z","relpermalink":"/publication/li-2007-techcon/","section":"publication","summary":"","tags":["conference"],"title":"Low Power and Variation Tolerant Digital Circuit Design in Sub-micron  Regime  using  Low  Cost LTPS TFTs","type":"publication"},{"authors":["Jing Li","Aditya Bansal","Kaushik Roy"],"categories":[],"content":"","date":1149120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1149120000,"objectID":"1adbe306827e0587bae3f25d62f73bc1","permalink":"https://penn-cil.github.io/publication/li-2006-drc/","publishdate":"2006-06-01T00:00:00Z","relpermalink":"/publication/li-2006-drc/","section":"publication","summary":"","tags":["conference"," costs","crystallization","dielectric substrates","digital circuits","fabrication","grain boundaries","grain size","silicon","temperature","thin film transistors"],"title":"Exploring Low Temperature Poly-Si for Low Cost and Low Power Sub-micron Digital Operation","type":"publication"}]