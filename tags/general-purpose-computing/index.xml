<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>general purpose computing | Penn Computational Intelligence Lab</title>
    <link>https://penn-cil.github.io/tags/general-purpose-computing/</link>
      <atom:link href="https://penn-cil.github.io/tags/general-purpose-computing/index.xml" rel="self" type="application/rss+xml" />
    <description>general purpose computing</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 16 Aug 2019 16:00:10 -0500</lastBuildDate>
    <image>
      <url>https://penn-cil.github.io/img/logo.png</url>
      <title>general purpose computing</title>
      <link>https://penn-cil.github.io/tags/general-purpose-computing/</link>
    </image>
    
    <item>
      <title>Liquid Silicon</title>
      <link>https://penn-cil.github.io/project/liquid-silicon/</link>
      <pubDate>Fri, 16 Aug 2019 15:59:00 -0500</pubDate>
      <guid>https://penn-cil.github.io/project/liquid-silicon/</guid>
      <description>&lt;p&gt;Liquid Silicon (L-Si) is a general-purpose in-memory computing architecture with complete system support that addresses several key fundamental limitations of state-of-the-art reconfigurable data-flow architectures (including FPGA, TPU, CGRA, etc.) in supporting emerging machine learning and big data applications. As compared with most projects in literature which focus on part of the system stack, L-Si is a &lt;em&gt;full stack&lt;/em&gt; solution that comprises architecture [&lt;a href=&#34;https://penn-cil.github.io/publication/zha-2018-fpga&#34;&gt;Zha2018FPGA&lt;/a&gt;], compiler [&lt;a href=&#34;https://penn-cil.github.io/publication/zha-2016-iccad&#34;&gt;Zha2016ICCAD&lt;/a&gt;], programming model and system integration [&lt;a href=&#34;https://penn-cil.github.io/publication/zha-2018-asplos&#34;&gt;Zha2018ASPLOS&lt;/a&gt;], with a real chip demonstration [&lt;a href=&#34;https://penn-cil.github.io/publication/zha-2019-vlsic&#34;&gt;Zha2019VLSIC&lt;/a&gt;]. The computing model of L-Si is radically different from state-of-the-art reconfigurable data-flow architectures. It maximally reuses the memory array itself (instead of placing computation units near the array) to perform a) heavy weight computation (&lt;em&gt;logic&lt;/em&gt;), b) light weight computation(&lt;em&gt;search&lt;/em&gt;), c) data storage (&lt;em&gt;memory&lt;/em&gt;), and d) communication (&lt;em&gt;routing&lt;/em&gt;), with minimal requirement in CMOS supporting circuitry, which can thus be further placed beneath the array. Therefore, it inherits the great benefits of semiconductor memory in integration density and cost, and offers orders of magnitude more parallel data processing capability in situ in the memory array than the best-known solution today. For the &lt;strong&gt;first time&lt;/strong&gt;, it fundamentally blurs the boundary between computation and storage, by exploiting a continuum of general-purpose in-memory compute capabilities across the whole spectrum, from full memory to full computation, or intermediate states in between (partial memory and partial computation). Thus, it provides programmers (or compiler) more flexibility to customize hardware’s compute and memory resources to better match applications needs for higher performance and energy efficiency. We leverage such unique property and provide compiler support to facilitate the programming of applications written in high-level programming languages (e.g. OpenCL) and frameworks (e.g. TensorFlow, MapReduce) while fully exploiting the unique architectural capability of L-Si. We also provide scalable multi-context architectural support to minimize reconfiguration overhead for assisting virtualization when combined with our system stack.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://penn-cil.github.io/img/LSi.png&#34; alt=&#34;L-Si timeline&#34; title=&#34;Timeline of Liquid Silicon project&#34;&gt;&lt;em&gt;Timeline of L-Si project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To prove the feasibility of L-Si, we fabricated a test chip in 130 nm CMOS process with HfO2 RRAM – the &lt;strong&gt;first real-chip demonstration for general purpose in-memory computing using RRAM&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://penn-cil.github.io/img/LSi-VLSI.png&#34; alt=&#34;L-Si die photo&#34; title=&#34;Die Photo and Chip Characteristics of L-Si&#34;&gt;&lt;em&gt;Die Photo and Chip Characteristics of L-Si&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With proposed system support, we evaluated a broad class of legacy and emerging machine learning workloads. Our measurement confirmed the chip operates reliably at low voltage of 650 mV when running these workloads. It achieves 60.9 TOPS/W in performing neural network inferences and 480 GOPS/W in performing high-dimensional similarity search (a key big data application) at nominal voltage supply of 1.2V, showing &lt;strong&gt;&amp;gt; 3x&lt;/strong&gt; and &lt;strong&gt;~100x&lt;/strong&gt; power efficiency improvement over the state-of-the-art domain-specific CMOS-/RRAM-based accelerators without sacrificing the programmability. In addition, it outperforms the latest nonvolatile FPGA in energy efficiency by &lt;strong&gt;&amp;gt; 3x&lt;/strong&gt; in general compute-intensive applications. As L-Si is a fundamental new computing technology, moving further, we will explore how to scale it up to warehouse computers and scale it down to IoT devices by further specializing the software/hardware stacks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://penn-cil.github.io/img/LSi-VLSI-Results.png&#34; alt=&#34;Comparing L-Si with State-of-the-Art&#34; title=&#34;Comparing L-Si with State-of-the-Art&#34;&gt;&lt;em&gt;Comparing L-Si with State-of-the-Art&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Two Dimensional Associative Processor (2D AP)</title>
      <link>https://penn-cil.github.io/project/two-d-ap/</link>
      <pubDate>Fri, 16 Aug 2019 16:00:10 -0500</pubDate>
      <guid>https://penn-cil.github.io/project/two-d-ap/</guid>
      <description>&lt;p&gt;The research project, titled &amp;ldquo;&lt;em&gt;Associative In-Memory Graph Processing Paradigm: Towards Tera-TEPS Graph Traversal In a Box&lt;/em&gt;&amp;quot;, won the &lt;strong&gt;NSF CAREER Award&lt;/strong&gt; in 2018. In this research, we developed a radically new computing paradigm, namely two-dimensional associative processing (2D AP) to further advance our previous FPGA-based graph processing architectures and fundamentally address their limitations. Mathematically, 2D AP is a new general-purpose computing model that exploits an extra dimension of parallelism (both intra-word and inter-word parallelism) to accelerate computation as compared with traditional AP which only exploit inter-word parallelism. It is particularly beneficial for massive-scale graph processing. For the first time, we provide a theoretical proof that 2D AP is inherently more efficient as measured by &amp;ldquo;&lt;em&gt;architecturally determined complexity&lt;/em&gt;&amp;rdquo; in runtime/area/energy than both von Neumann architecture and traditional AP paradigm in performing graph computation. We also provide detailed micro-architectures and circuits to best implement the proposed computing model, with domain-special language support. A preliminary published version of 2D AP [&lt;a href=&#34;https://penn-cil.github.io/publication/khoram-2018-cal&#34;&gt;Khoram2018CAL&lt;/a&gt;] was recognized as &lt;strong&gt;best of CAL&lt;/strong&gt; (IEEE Computer Architecture Letters) in 2018.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
