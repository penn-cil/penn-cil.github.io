@inproceedings{khoram2018iclr,
 abstract = {Despite the state-of-the-art accuracy of Deep Neural Networks (DNN) in various classification problems, their deployment onto resource constrained edge computing devices remains challenging due to their large size and complexity. Several recent studies have reported remarkable results in reducing this complexity through quantization of DNN models. However, these studies usually do not consider the changes in the loss function when performing quantization, nor do they take the different importances of DNN model parameters to the accuracy into account. We address these issues in this paper by proposing a new method, called adaptive quantization, which simplifies a trained DNN model by finding a unique, optimal precision for each network parameter such that the increase in loss is minimized. The optimization problem at the core of this method iteratively uses the loss function gradient to determine an error margin for each parameter and assigns it a precision accordingly. Since this problem uses linear functions, it is computationally cheap and, as we will show, has a closed-form approximate solution. Experiments on MNIST, CIFAR, and SVHN datasets showed that the proposed method can achieve near or better than state-of-the-art reduction in model size with similar error rates. Furthermore, it can achieve compressions close to floating-point model compression methods without loss of accuracy.},
 author = {Khoram, Soroosh and Li, Jing},
 booktitle = {International Conference on Learning Representations (**ICLR**)},
 date = {2018-04},
 keywords = {conference},
 month = {April},
 note = {(Acceptance Rate: \underline{34\%}, 314 out of 935)},
 title = {Adaptive Quantization of Neural Networks},
 url = {https://openreview.net/forum?id=SyOK1Sg0W},
 year = {2018}
}

